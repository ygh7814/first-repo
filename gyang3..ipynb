{
  "metadata": {
    "name": "gyang3",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\n/etc/profile.d/init-user-hdfs.sh"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nls"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nimport pandas as pd\nimport numpy as np\ndata \u003dpd.DataFrame({\u0027animal\u0027: [\u0027cat\u0027, \u0027cat\u0027, \u0027snake\u0027, \u0027dog\u0027, \u0027dog\u0027, \u0027cat\u0027, \u0027snake\u0027, \u0027cat\u0027, \u0027dog\u0027, \u0027dog\u0027],\n        \u0027age\u0027: [2.5, 3, 0.5, np.nan, 5, 2, 4.5, np.nan, 7, 3],\n        \u0027visits\u0027: [1, 3, 2, 3, 2, 3, 1, 1, 2, 1],\n        \u0027priority\u0027: [\u0027yes\u0027, \u0027yes\u0027, \u0027no\u0027, \u0027yes\u0027, \u0027no\u0027, \u0027no\u0027, \u0027no\u0027, \u0027yes\u0027, \u0027no\u0027, \u0027no\u0027]})\ndata\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\ndatas\u003dspark.createDataFrame(data)\ndatas.show(3)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndatas.describe().show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndatas.printSchema()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndatas[datas.age.between(2,4)].show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndatas.select(datas.columns[:3]).show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndatas.createOrReplaceTempView(\u0027datas_table\u0027)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nx\u003d20\ny\u003d30\nspark.sql(\"select src_ord_dt as Day,sum(dmd_amt) as Total_Dmd from us_core.fct_dmd_item \\\nwhere dmd_amt between {}  and {}  and year(src_ord_dt)\u003d2017 \\\ngroup by src_ord_dt \\\norder by 1 limit 100\".format(x,y)).show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.sql\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.sql\nshow functions\n"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndate1\u003d\u00272018-07-01\u0027\ndate2\u003d\u00272018-08-01\u0027\nspark.sql(\"select distinct vc.edw_hshld_num ,sum(fdi.REG_DMD_QTY) as Total_Units,sum(case when  (sku.gmm_num in (\u002701\u0027) or (sku.gmm_num \u003d \u002703\u0027 and sku.cat_num in (57))) then fdi.dmd_amt else 0 end) as Men_dmd ,\\\n          sum(case when (sku.gmm_num in (\u002702\u0027) or (sku.gmm_num \u003d \u002703\u0027 and sku.cat_num in (3,7,56,80))) then fdi.dmd_amt else 0 end) as WM_dmd,\\\n        sum(case when (sku.GMM_NUM \u003d \u002706\u0027) or (sku.GMM_NUM \u003d \u002703\u0027 and sku.cat_num \u003d 39) then fdi.dmd_amt else 0 end) as Kids_dmd \\\n\t    ,sum(case when (sku.GMM_NUM \u003d \u002707\u0027) then fdi.dmd_amt else 0 end) as Home_dmd  \\\n\t    ,sum(case when (sku.GMM_NUM \u003d \u002705\u0027) then fdi.dmd_amt else 0 end) as Uniform_dmd012 \\\n\t    ,sum(case when sku.GMM_NUM not in (\u002701\u0027,\u002702\u0027,\u002703\u0027,\u002706\u0027,\u002707\u0027,\u002705\u0027) then fdi.dmd_amt else 0 end) as Other_dmd \\\n\tFROM   core_netezza.vw_consumer as vc\\\n\t       inner join core_netezza.FCT_DMD_ITEM fdi \\\n\t        on vc.cnsmr_num\u003dfdi.cnsmr_num \\\n\t        inner join core_netezza.SKU_2018_06_26 sku    \\\n\t\t  \ton sku.SKU_NUM \u003d fdi.sku_num   \\\n    where fdi.reg_dmd_qty \u003e 0 and fdi.bus_unit_num \u003d 1 and fdi.src_ord_Dt between \u0027{}\u0027  and \u0027{}\u0027  \\\n    group by 1  \".format(date1,date2)).show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"Select coalesce(dp.EDW_HSHLD_NUM,rp.EDW_HSHLD_NUM) as EDW_HSHLD_NUM\t\\\n                ,case when dp.EDW_HSHLD_NUM is not null and rp.EDW_HSHLD_NUM is not null then \u0027D+R\u0027 \\\n\t             when dp.EDW_HSHLD_NUM is null and rp.EDW_HSHLD_NUM is not null then \u0027RO\u0027 \\\n\t             when dp.EDW_HSHLD_NUM is not null and rp.EDW_HSHLD_NUM is null then \u0027DO\u0027 \\\n\t              else \u0027Err\u0027 end as buyerType    \\\n,coalesce(Direct_dmd,0) as Direct_dmd \\\n,coalesce(Retail_dmd,0) as Retail_dmd  \\\n,coalesce(Direct_dmd,0)+coalesce(Retail_dmd,0) as Total_Demand \\\n,case when (coalesce(Direct_dmd,0)+coalesce(Retail_dmd,0))\u003e0 then coalesce(Direct_dmd,0)/(coalesce(Direct_dmd,0)+coalesce(Retail_dmd,0)) else 0 end as Direct_pct \\\n,case when (coalesce(Direct_dmd,0)+coalesce(Retail_dmd,0))\u003e0 then coalesce(Retail_dmd,0)/(coalesce(Direct_dmd,0)+coalesce(Retail_dmd,0)) else 0 end as Retail_pct \\\nFrom (select vc.EDW_HSHLD_NUM,sum(fdi.dmd_amt) as Direct_dmd \\\n\t\t\tFROM core_netezza.FCT_DMD_ITEM FDI   \\\n\t\t\t\tinner join core_netezza.vw_consumer VC \\\n\t\t\t\t\ton vc.cnsmr_num \u003d fdi.cnsmr_num  \\\n\t\t\tWHERE FDI.BUS_UNIT_NUM \u003d 1\tAND FDI.REG_DMD_QTY \u003e 0\tand fdi.src_ord_dt between \u00272018-07-01\u0027 and  \u00272018-08-01\u0027 \\\n\t\t\tgroup by 1) as DP  \\\nfull outer join  \\\n\t(select vc.EDW_HSHLD_NUM,sum(i.sell_amt + i.oth_disc_amt) as Retail_dmd \\\n\tFROM core_netezza.S_RTL_TRAN_ITEM_2018_05_23            I\\\n\t\tinner join core_netezza.S_RTL_TRAN_2018_05_23       R \\\n\t\t\ton R.S_FAC_NUM \u003dI.S_FAC_NUM   AND R.REG_NUM\u003dI.REG_NUM               \\\n\t\t\tAND R.TRAN_NUM\u003dI.TRAN_NUM    \\\n\t\t\tAND R.TRAN_DT \u003d I.TRAN_DT     \\\n\t\t\tAND R.TRAN_Ts\u003dI.TRAN_Ts       \\\n\t\tinner join core_netezza.vw_consumer VC   \\\n\t\t\ton vc.cnsmr_num \u003d R.cnsmr_num     \\\n\twhere I.ITEM_TRAN_TYP_CD IN (\u0027S\u0027) and vc.EDW_HSHLD_NUM is not null and I.TRAN_DT between \u00272018-07-01\u0027 and  \u00272018-08-01\u0027 \\\n\tgroup by 1) as RP      \\\n\t\ton DP.EDW_HSHLD_NUM \u003d RP.EDW_HSHLD_NUM\").show() \n"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"select * from core_netezza.vw_consumer limit 10 \" ).show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndate\u003d\"08-01-2018\"\nspark.sql(\"select distinct aci.edw_hshld_num,avg(round(months_between(to_date(concat(aci.acxm_8624_input_indv_birth_mm,\u0027-15-\u0027,aci.acxm_8624_input_indv_birth_yr),\u0027mm-dd-yyy\u0027),to_date(\u0027{}\u0027,\u0027mm-dd-yy\u0027))/12)) as age \\\n           ,avg(ach.acxm_7641_est_hshld_inc_cd) as income from core_netezza.acxm_cust_indv_2018_05_21 as aci join core_netezza.acxm_cust_hshld_2018_05_21 as ach on aci.edw_hshld_num\u003dach.edw_hshld_num \\\n           group by 1 limit 10\".format(date)).show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"select distinct vc.cnsmr_num,sum(case when  (sku.gmm_num in (\u002701\u0027) or (sku.gmm_num \u003d \u002703\u0027 and sku.cat_num in (57))) then fdi.dmd_amt else 0 end) as Men_dmd \\\n           FROM  core_netezza.VW_CONSUMER  as VC  \\\n\t       inner join core_netezza.FCT_DMD_ITEM fdi \\\n\t        on vc.cnsmr_num \u003d fdi.cnsmr_num \\\n\t        inner join core_netezza.SKU_2018_06_26 sku    \\\n\t\t  \ton sku.SKU_NUM \u003d fdi.sku_num  \\\n           where bus_unit_num\u003d1 and src_ord_dt between \u00272018-07-01\u0027  and \u00272018-08-01\u0027 \\\n           group by 1 \\\n           limit 10\").show()"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nspark.sql(\"select distinct aci.edw_hshld_num,avg(trunc(months_between(\u00272018-01-01\u0027,to_date(concaci.acxm_8624_input_indv_birth_mm||\u0027-15-\u0027||aci.acxm_8624_input_indv_birth_yr,\u0027mm-dd-yyyy\u0027)) / 12)) as age \\\n                   ,avg(ach.acxm_7641_est_hshld_inc_cd) as income   from core_netezza.acxm_cust_indv as aci \\\n                   join core_netezza.acxm_cust_hshld               as ach \\\n                  on aci.edw_hshld_num \u003d ach.edw_hshld_num   \\\n    \t\t\t  group by 1                                 \\\n    \t\t\t  limit 10\").show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\n\nemrfs sync s3://lerefinedzone \u003e\u00261\n"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n#1\ncutoff_date,anticipate_bdate,anticipate_edate,independent_bdate60plus,independent_edate60plus,independent_bdate60,independent_edate60,independent_edate13,independent_bdate12,web_date\u003d\u00272017-10-13\u0027, \u00272017-11-19\u0027, \u00272017-12-18\u0027, \u00272006-01-01\u0027,\u00272012-10-13\u0027, \u00272012-10-13\u0027, \u00272017-10-12\u0027, \u00272016-10-12\u0027, \u00272016-10-13\u0027, \u00272017-09-13\u0027"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ncutoff_date"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n#2\ndata_query\u003d\"select distinct vc.edw_hshld_num from us_core.FCT_CTLG_OFFR fco  \\\n                             inner join us_core.vw_consumer vc    \\\n                                   on vc.CNSMR_NUM\u003dfco.cnsmr_num \\\n                                       inner join us_core.cnsmr_sum as cs \\\n                                            on vc.cnsmr_num \u003d cs.cnsmr_num \\\n                \t\t\t\t\t   \t         where fco.seg_cd\u003d\u0027U11BN\u0027 and iss_id\u003d\u0027C500\u0027 and iss_yr\u003d2018 and cs.CNTRY_NUM\u003d840\"\ndepend_query\u003d\"select m1.edw_hshld_num,coalesce(m2.PH_IA_dmd,0) dmd from \\\n                    (select distinct vc.EDW_HSHLD_NUM from us_core.FCT_CTLG_OFFR fco  \\\n                    inner join us_core.vw_consumer vc   \\\n                    on vc.CNSMR_NUM\u003dfco.cnsmr_num \\\n                    where fco.seg_cd\u003d\u0027U11BN\u0027 and iss_id\u003d\u0027C500\u0027 and iss_yr\u003d2018) m1 \\\n                    left join( \\\n                    select coalesce(ph.edw_hshld_num,ia.edw_hshld_num) as edw_hshld_num,coalesce(ph.ph_dmd,0)+coalesce(IA.IA_dmd,0) as PH_IA_dmd \\\n                    from   \\\n                    (select vc.edw_hshld_num,sum(FDI.DMD_AMT) as ph_dmd  \\\n                    \tfrom us_core.FCT_DMD_ITEM fdi    \\\n                    \t\tinner join us_core.VW_CONSUMER vc   \\\n                    \t\t\ton vc.cnsmr_num \u003d fdi.cnsmr_num  \\\n                    \t\tinner join us_core.ISSUE i    \\\n                    \t\t\ton fdi.ISS_NUM \u003d I.ISS_NUM   \\\n                    \t\t\tand FDI.BUS_UNIT_NUM \u003d I.BUS_UNIT_NUM  \\\n                    \t\t\tand fdi.ISS_ID \u003d I.ISS_ID   \\\n                    \t\t\tand fdi.ISS_YR \u003d I.ISS_YR   \\\n                    \t\t\tand fdi.year in (2017)    \\\n                    \t\t\tand fdi.REG_DMD_QTY \u003e 0\tand fdi.BUS_UNIT_NUM \u003d 1 and fdi.SRC_ORD_DT between \u0027{}\u0027 and \u0027{}\u0027 \\\n                    \twhere I.RESP_ISS_TYP_CD !\u003d \u002707\u0027\tand I.BUS_UNIT_NUM \u003d 1 and I.ISS_ID in (\u0027C500\u0027) and I.ISS_YR \u003d 2018  \\\n                    \t \tgroup by 1  \\\n                    \t) PH   \\\n                    full join  \\\n                    \t(select vc.edw_hshld_num,sum(IA.IA_DMD_AMT) as IA_dmd  \\\n                    \tfrom us_core.FCT_DMD_ITEM fdi \\\n                    \t    inner join us_core.VW_CONSUMER vc \\\n                    \t\t\ton vc.cnsmr_num \u003d fdi.cnsmr_num  \\\n                    \t\t\tand fdi.year in (2017)\\\n                    \t\tinner join us_core.ORD_ITEM_IA_2 IA   \\\n                    \t\t\ton fdi.SRC_ORD_DT  \u003d IA.ORD_DT  \\\n                    \t\t\tand fdi.SRC_ORD_NUM \u003d IA.ORD_NUM  \\\n                    \t\t\tand fdi.ITEM_SEQ_NUM \u003d IA.ITEM_SEQ_NUM \\\n                    \t\tinner join us_core.ISSUE i  \\\n                    \t\t\ton IA.IA_ISS_ID \u003d I.ISS_ID \\\n                    \t\t\tand IA.IA_ISS_YR \u003d I.ISS_YR  \\\n                    \t\twhere fdi.REG_DMD_QTY \u003e 0 and fdi.BUS_UNIT_NUM \u003d 1  and IA.MED_TYP_CD not in (12, 13) and I.BUS_UNIT_NUM \u003d 1 and IA_ISS_ID in (\u0027C500\u0027) and IA_ISS_YR \u003d 2018 \\\n                    \t\tand fdi.SRC_ORD_DT between \u0027{}\u0027 and \u0027{}\u0027   \\\n                              \tgroup by 1 ) IA \\\n                    \ton  PH.edw_hshld_num \u003d IA.edw_hshld_num) m2 \\\n                    on m1.edw_hshld_num\u003dm2.edw_hshld_num \"\ndep_sql\u003d\"select distinct vc.edw_hshld_num,case when a.dmd is not null then a.dmd else 0 end as dmd,c.income,c.age from   (\"+data_query+\") vc  \\\n    \t\tleft join (\"+depend_query+\") a  \\\n    \t\t\ton vc.edw_hshld_num\u003da.edw_hshld_num    \\\n    \t\t\tleft join  \\\n                (select distinct  aci.edw_hshld_num,avg(round(months_between(to_date(\u0027{}\u0027,\u0027yyyy-mm-dd\u0027),to_date(concat(aci.acxm_8624_input_indv_birth_yr,\u0027-15-\u0027,aci.acxm_8624_input_indv_birth_mm),\u0027yyyy-mm-dd\u0027))/12)) as age  \\\n                   ,avg(ach.acxm_7641_est_hshld_inc_cd)  as income  \\\n                   from core_netezza.acxm_cust_indv_2018_05_21 as aci \\\n                   join core_netezza.acxm_cust_hshld_2018_05_21 as ach  \\\n                  on aci.edw_hshld_num \u003d ach.edw_hshld_num   \\\n    \t\t\t  group by 1) c \\\n           on vc.edw_hshld_num\u003dc.edw_hshld_num\"\n"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\u0027select * from us_core.\u0027).show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndmd_order_sql\u003d\"select vc.edw_hshld_num,sum(fdi.REG_DMD_QTY) as Total_Units,sum(case when  (sku.gmm_num in (\u002701\u0027) or (sku.gmm_num \u003d \u002703\u0027 and sku.cat_num in (57))) \\\n               then fdi.dmd_amt else 0 end) as Men_dmd, count(distinct case when  (sku.gmm_num in (\u002701\u0027) or (sku.gmm_num \u003d \u002703\u0027 and sku.cat_num in (57))) then concat(FDI.SRC_ORD_DT,FDI.SRC_ORD_NUM) else null end) Men_order,sum(case when (sku.gmm_num in (\u002702\u0027) or (sku.gmm_num \u003d \u002703\u0027 and sku.cat_num in (3,7,56,80))) then fdi.dmd_amt else 0 end) as WM_dmd, count(distinct case when  (sku.gmm_num in (\u002702\u0027) or (sku.gmm_num \u003d \u002703\u0027 and sku.cat_num in (3,7,56,80))) then concat(FDI.SRC_ORD_DT,FDI.SRC_ORD_NUM) else null end) WM_order,sum(case when (sku.GMM_NUM \u003d \u002706\u0027) or (sku.GMM_NUM \u003d \u002703\u0027 and sku.cat_num \u003d 39) then fdi.dmd_amt else 0 end) as Kids_dmd,\\\n        count(distinct case when  (sku.GMM_NUM \u003d \u002706\u0027) or (sku.GMM_NUM \u003d \u002703\u0027 and sku.cat_num \u003d 39) then concat(FDI.SRC_ORD_DT,FDI.SRC_ORD_NUM) else null end) Kids_order \\\n\t    ,sum(case when (sku.GMM_NUM \u003d \u002707\u0027) then fdi.dmd_amt else 0 end) as Home_dmd  \\\n        ,count(distinct case when   (sku.GMM_NUM \u003d \u002707\u0027)  then concat(FDI.SRC_ORD_DT,FDI.SRC_ORD_NUM) else null end) Home_order \\\n\t    ,sum(case when (sku.GMM_NUM \u003d \u002705\u0027) then fdi.dmd_amt else 0 end) as Uniform_dmd012 \\\n        ,count(distinct case when   (sku.GMM_NUM \u003d \u002705\u0027)  then concat(FDI.SRC_ORD_DT,FDI.SRC_ORD_NUM) else null end) Uniform_order012 \\\n\t    ,sum(case when sku.GMM_NUM not in (\u002701\u0027,\u002702\u0027,\u002703\u0027,\u002706\u0027,\u002707\u0027,\u002705\u0027) then fdi.dmd_amt else 0 end) as Other_dmd \\\n         ,count(distinct case when  sku.GMM_NUM not in (\u002701\u0027,\u002702\u0027,\u002703\u0027,\u002706\u0027,\u002707\u0027,\u002705\u0027)  then concat(FDI.SRC_ORD_DT,FDI.SRC_ORD_NUM) else null end) Other_order \\\n         FROM (\"+data_query+\")   as usr \\\n         inner join us_core.VW_CONSUMER as VC  \\\n\t\t\t  on usr.edw_hshld_num \u003d vc.edw_hshld_num\\\n\t    inner join us_core.FCT_DMD_ITEM fdi   \\\n\t    on vc.cnsmr_num \u003d fdi.cnsmr_num  and  fdi.year in(2012,2013,2014,2015,2016,2017) \\\n\t    and fdi.reg_dmd_qty \u003e 0 and fdi.bus_unit_num \u003d 1 and fdi.src_ord_Dt between \u0027{}\u0027 and \u0027{}\u0027 \\\n\t    inner join us_core.SKU sku  \\\n\t    on sku.SKU_NUM \u003d fdi.sku_num  \\\n\t    group by 1\""
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndmd_order\u003dspark.sql(dmd_order_sql.format(independent_bdate60,independent_edate60))"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndmd_order.count()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ntotal_dmd_sql\u003d\"Select coalesce(dp.edw_hshld_num,rp.edw_hshld_num) as edw_hshld_num,case when dp.EDW_HSHLD_NUM is not null and rp.EDW_HSHLD_NUM is not null then \u0027D+R\u0027 when dp.EDW_HSHLD_NUM is null and rp.EDW_HSHLD_NUM is not null then \u0027RO\u0027 \\\n\t             when dp.EDW_HSHLD_NUM is not null and rp.EDW_HSHLD_NUM is null then \u0027DO\u0027 else \u0027Err\u0027 end as buyerType ,coalesce(Direct_dmd,0) as Direct_dmd ,coalesce(Retail_dmd,0) as Retail_dmd ,coalesce(Direct_dmd,0)+coalesce(Retail_dmd,0) as Total_Demand,case when (coalesce(Direct_dmd,0)+coalesce(Retail_dmd,0))\u003e0 then coalesce(Direct_dmd,0)/(coalesce(Direct_dmd,0)+coalesce(Retail_dmd,0)) else 0 end as Direct_pct,case when (coalesce(Direct_dmd,0)+coalesce(Retail_dmd,0))\u003e0 then coalesce(Retail_dmd,0)/(coalesce(Direct_dmd,0)+coalesce(Retail_dmd,0)) else 0 end as Retail_pct\\\n                From (select vc.EDW_HSHLD_NUM,sum(fdi.dmd_amt) as Direct_dmd  FROM us_core.FCT_DMD_ITEM FDI\\\n\t\t\t\tinner join us_core.vw_consumer VC on vc.cnsmr_num \u003d fdi.cnsmr_num and fdi.year in (2012,2013,2014,2015,2016,2017) \\\n\t\t\t\tand FDI.BUS_UNIT_NUM \u003d 1 AND FDI.REG_DMD_QTY \u003e 0\tand fdi.src_ord_dt between \u0027{}\u0027 and \u0027{}\u0027\\\n\t\t\t\tinner join (\"+data_query+\")   as usr \\\n\t  \t\t\t\ton usr.edw_hshld_num \u003d vc.edw_hshld_num  \\\n\t  \t\t\t\t  \\\n\t\t\t    group by 1) as DP \\\n              full outer join \\\n\t          (select vc.EDW_HSHLD_NUM,sum(i.sell_amt) as Retail_dmd \\\n\t           FROM us_core.S_RTL_TRAN_ITEM            I \\\n\t\t       inner join us_core.S_RTL_TRAN       R  \\\n    \t\t\ton R.S_FAC_NUM \u003dI.S_FAC_NUM       \\\n    \t\t\tAND R.REG_NUM\u003dI.REG_NUM       \\\n    \t\t\tAND R.TRAN_NUM\u003dI.TRAN_NUM    \\\n    \t\t\tAND R.TRAN_DT \u003d I.TRAN_DT   \\\n    \t\t\tAND R.TRAN_TS\u003dI.TRAN_TS    \\\n    \t\t\tAND R.year in (2012,2013,2014,2015,2016,2017) \\\n    \t\t\tAND I.ITEM_TRAN_TYP_CD IN (\u0027S\u0027) and I.TRAN_DT between \u0027{}\u0027 and \u0027{}\u0027\\\n\t\t        inner join us_core.vw_consumer VC  \\\n        \t\t\ton vc.cnsmr_num \u003d R.cnsmr_num  \\\n        \t\tinner join  (\"+data_query+\")    as usr \\\n        \t  on usr.edw_hshld_num \u003d vc.edw_hshld_num\t\\\n        \twhere vc.EDW_HSHLD_NUM is not null \\\n        \tgroup by 1) as RP                \\\n        \t\ton DP.EDW_HSHLD_NUM \u003d RP.EDW_HSHLD_NUM\"\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ntotal_dmd\u003dspark.sql(total_dmd_sql.format(independent_bdate60,independent_edate60,independent_bdate60,independent_edate60))\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nemrfs sync s3://lerefinedzone/core/netezza \u003e/dev/null\necho $?\n"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n#3\ndepend\u003dspark.sql(dep_sql.format(anticipate_bdate,anticipate_edate,anticipate_bdate,anticipate_edate,independent_edate60))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nrecency_sql\u003d\"select nd.edw_hshld_num,case when a.recency is not NULL then a.recency else 132 end as recency  from (\"+data_query+\")  as nd left join ( \\\n\t\t\t  select distinct vc.edw_hshld_num ,round(months_between(\u0027{}\u0027,max(SRC_ORD_DT))) as recency from us_core.VW_CONSUMER vc \\\n\t\t\t    inner join us_core.FCT_DMD_ITEM fdi \\\n\t\t\t    on vc.cnsmr_num \u003d fdi.cnsmr_num and  fdi.year in (2012,2013,2014,2015,2016,2017) and \\\n                 fdi.bus_unit_num \u003d 1  and fdi.reg_dmd_qty\u003e 0 and fdi.dmd_amt\u003e0 and fdi.src_ord_Dt between \u0027{}\u0027 and \u0027{}\u0027 \\\n                    group by 1) a \\\n\t\t\t     on nd.edw_hshld_num\u003da.edw_hshld_num\"\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nrecency\u003dspark.sql(recency_sql.format(independent_edate60,independent_bdate60,independent_edate60))"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nd.count()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n\nselect vc.edw_hshld_num, sum(Emails_clicked) as Emails_clicked from (select distinct edw_hshld_num from (select edw_hshld_num,  case when recent between \u00272012-01-06\u0027 and \u00272017-01-06\u0027 then \u00270-60\u0027 when recent between \u00272006-01-01\u0027 and \u00272012-01-06\u0027 then \u002761+\u0027 \n            else \u0027Other\u0027 end as recency, case when ords \u003d 1 then \u00271\u0027 when ords \u003e\u003d 2 then \u00272X\u0027 else \u0027Error\u0027 end as freq from \n            (select vc.edw_hshld_num, max(fdi.src_ord_dt) as recent, COUNT(DISTINCT concat(FDI.SRC_ORD_DT,FDI.SRC_ORD_NUM)) AS ords \n                FROM  us_core.VW_CONSUMER vc \n                    inner join us_core.FCT_DMD_ITEM fdi \n                       on vc.cnsmr_num \u003d fdi.cnsmr_num \n                            where fdi.reg_dmd_qty \u003e 0 and fdi.bus_unit_num \u003d 1 and fdi.src_ord_Dt between \u00272006-01-01\u0027 and \u00272017-01-06\u0027  \n                                    group by 1) A)B \n            where recency \u003d \u002761+\u0027 )  as usr inner join us_core.vw_consumer VC \n\t             on usr.edw_hshld_num \u003d vc.edw_hshld_num   inner join  \n\t           (select c.CNSMR_NUM, count(distinct concat(eo.EMAIL_OFFR_NUM,eo.CAMP_EVNT_NUM,eo.EVNT_DT)) as Emails_clicked \n\t\t   FROM us_core.CONSUMER                  C \n\t\t    inner join us_core.cust_cntct_pt_active     CCP \n\t\t\t\ton C.CUST_ID \u003d CCP.CUST_ID                         \n\t\t\tinner join us_core.EMAIL_OFFR              EO     \n\t\t\t\ton CCP.CUST_CNTCT_PT_NUM \u003d EO.CUST_CNTCT_PT_NUM \n\t\t\t     \n\t\t  where  EO.EVNT_TYP_CD \u003d 1 and EO.EVNT_METH_CD \u003d 6 \n\t\t  group by 1) A  \n        on A.cnsmr_num \u003d vc.cnsmr_num \n         group by 1   \n         order by 1 \n         \n    "
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n\nSELECT count(distinct email_offr_num) from email_offer\n"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ntotal_catalog_sql\u003d\"select vc.edw_hshld_num,count(distinct case when fco.mail_dt between \u0027{}\u0027 and \u0027{}\u0027 then concat(fco.iss_id,fco.iss_yr) else null end) as Tot1360_Catalogs \\\n                           ,count(distinct case when fco.mail_dt between \u0027{}\u0027 and \u0027{}\u0027 then concat(fco.iss_id,fco.iss_yr) else null end) as Tot012_Catalogs  \\\n                                 from us_core.FCT_CTLG_OFFR fco   \\\n\tinner join us_core.VW_CONSUMER vc  \\\n\t\ton vc.CNSMR_NUM \u003d fco.cnsmr_num \\\n\t\tand fco.year in (2012,2013,2014,2015,2016,2017) \\\n\t\tand fco.BUS_UNIT_NUM \u003d 1 and fco.mail_dt between \u0027{}\u0027 and \u0027{}\u0027  \\\n\tinner join (\"+data_query+\")    as usr  \\\n\t  on usr.edw_hshld_num \u003d vc.edw_hshld_num \\\n\tinner join us_core.ISSUE I   \\\n\t\ton fCO.ISS_NUM \u003d I.ISS_NUM   \\\n\t\tand fco.ISS_ID \u003d i.ISS_ID   \\\n\t\tand fco.ISS_YR \u003d i.ISS_YR   \\\n\t\tand fco.BUS_UNIT_NUM \u003d I.BUS_UNIT_NUM  \\\n\t\tgroup by 1\"\n        \n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ntotal_catalog\u003dspark.sql(total_catalog_sql.format(independent_bdate60,independent_edate13,independent_bdate12,independent_edate60,independent_bdate60,independent_edate60))"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ncore_catalog_sql\u003d\"select vc.edw_hshld_num,count(distinct case when fco.mail_dt between \u0027{}\u0027 and \u0027{}\u0027 then concat(fco.iss_id,fco.iss_yr) else null end) as Core1360_Catalogs \\\n                    ,count(distinct case when fco.mail_dt between \u0027{}\u0027 and \u0027{}\u0027 then concat(fco.iss_id,fco.iss_yr) else null end) as Core012_Catalogs\\\n                    from us_core.FCT_CTLG_OFFR fco \\\n                    inner join us_core.VW_CONSUMER vc  \\\n\t\ton vc.CNSMR_NUM \u003d fco.cnsmr_num \\\n\t\tand  fco.BUS_UNIT_NUM \u003d 1 and fco.mail_dt between \u0027{}\u0027 and \u0027{}\u0027 \\\n\t\tand fco.year in (2012,2013,2014,2015,2016,2017)\\\n\tinner join (\"+data_query+\")   as usr \\\n\t  on usr.edw_hshld_num \u003d vc.edw_hshld_num  \\\n\tinner join us_core.ISSUE I    \\\n\t\ton fCO.ISS_NUM \u003d I.ISS_NUM  \\\n\t\tand fco.ISS_ID \u003d i.ISS_ID    \\\n\t\tand fco.ISS_YR \u003d i.ISS_YR   \\\n\t\tand fco.BUS_UNIT_NUM \u003d I.BUS_UNIT_NUM \\\n        and I.RESP_ISS_TYP_CD \u003d \u002701\u0027 \\\n        group by 1\"\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ncore_catalog\u003dspark.sql(core_catalog_sql.format(independent_bdate60,independent_edate13,independent_bdate12,independent_edate60,independent_bdate60,independent_edate60))"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nemrfs sync s3://lerefinedzone/us/mapping/FSCL_CAL_DT \u003e/dev/null\necho $?\n"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nemail_click_sql\u003d\"select vc.edw_hshld_num, sum(Emails_clicked) as Emails_clicked from (\"+data_query+\")  as usr inner join us_core.vw_consumer VC \\\n\t             on usr.edw_hshld_num \u003d vc.edw_hshld_num   inner join  \\\n\t           (select c.CNSMR_NUM, count(distinct concat(eo.EMAIL_OFFR_NUM,eo.CAMP_EVNT_NUM,eo.EVNT_DT)) as Emails_clicked \\\n\t\t   FROM us_core.CONSUMER                  C \\\n\t\t        inner join us_core.EMAIL_OFFR_RSLT_DTL_AGG DA \\\n\t\t         on c.cnsmr_num\u003dDA.cnsmr_num and da.year in(2017) \\\n\t\t\t    inner join us_core.email_offr             EO     \\\n\t\t\t     on DA.OUTBND_CUST_EVNT_NUM \u003d EO.OUTBND_CUST_EVNT_NUM \\\n\t\t\t\t    and DA.EVNT_TYP_CD \u003d EO.EVNT_TYP_CD   \\\n                    and RSLT_CD \u003d 10 AND DA.RSLT_DT between \u0027{}\u0027 and \u0027{}\u0027 \\\n                \t  group by 1) A  \\\n          on A.cnsmr_num \u003d vc.cnsmr_num \\\n         group by 1 \"\n         \nemail_clicks\u003dspark.sql(email_click_sql.format(web_date,independent_edate60))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nemail_open_sql\u003d\"select vc.edw_hshld_num, sum(Emails_opened) as Emails_opened from (\"+data_query+\")  as usr inner join us_core.vw_consumer VC \\\n\t             on usr.edw_hshld_num \u003d vc.edw_hshld_num   inner join  \\\n\t           (select c.CNSMR_NUM, count(distinct concat(eo.EMAIL_OFFR_NUM,eo.CAMP_EVNT_NUM,eo.EVNT_DT)) as Emails_opened \\\n\t\t   FROM us_core.CONSUMER                  C \\\n\t\t        inner join us_core.EMAIL_OFFR_RSLT_DTL_AGG DA \\\n\t\t         on c.cnsmr_num\u003dDA.cnsmr_num and da.year in (2017) \\\n\t\t\t    inner join us_core.email_offr             EO     \\\n\t\t\t     on DA.OUTBND_CUST_EVNT_NUM \u003d EO.OUTBND_CUST_EVNT_NUM \\\n\t\t\t\t    and DA.EVNT_TYP_CD \u003d EO.EVNT_TYP_CD  and EO.year in (2017) \\\n                    and RSLT_CD \u003d 8 AND DA.RSLT_DT between \u0027{}\u0027 and \u0027{}\u0027 \\\n                \t  group by 1) A  \\\n          on A.cnsmr_num \u003d vc.cnsmr_num \\\n         group by 1 \"\n         \nemail_opens\u003dspark.sql(email_open_sql.format(web_date,independent_edate60))"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\norder_time_sql\u003d\"select vc.edw_hshld_num \\\n                ,count(distinct concat(fdi.SRC_ORD_DT,fdi.src_ord_num)) as Total_Orders \\\n                ,count(distinct case when fy.FSCL_YR_MM\u003d1  then concat(fdi.SRC_ORD_DT,fdi.src_ord_num) else null end)as FM_Feb_Orders \\\n                ,count(distinct case when fy.FSCL_YR_MM\u003d2  then concat(fdi.SRC_ORD_DT,fdi.src_ord_num) else null end)as FM_Mar_Orders \\\n                ,count(distinct case when fy.FSCL_YR_MM\u003d3  then concat(fdi.SRC_ORD_DT,fdi.src_ord_num) else null end)as FM_Apr_Orders \\\n                ,count(distinct case when fy.FSCL_YR_MM\u003d4  then concat(fdi.SRC_ORD_DT,fdi.src_ord_num) else null end)as FM_May_Orders \\\n                ,count(distinct case when fy.FSCL_YR_MM\u003d5  then concat(fdi.SRC_ORD_DT,fdi.src_ord_num) else null end)as FM_Jun_Orders \\\n                ,count(distinct case when fy.FSCL_YR_MM\u003d6  then concat(fdi.SRC_ORD_DT,fdi.src_ord_num) else null end)as FM_Jul_Orders \\\n                ,count(distinct case when fy.FSCL_YR_MM\u003d7  then concat(fdi.SRC_ORD_DT,fdi.src_ord_num) else null end)as FM_Aug_Orders \\\n                ,count(distinct case when fy.FSCL_YR_MM\u003d8  then concat(fdi.SRC_ORD_DT,fdi.src_ord_num) else null end)as FM_Sep_Orders \\\n                ,count(distinct case when fy.FSCL_YR_MM\u003d9  then concat(fdi.SRC_ORD_DT,fdi.src_ord_num) else null end)as FM_Oct_Orders \\\n                ,count(distinct case when fy.FSCL_YR_MM\u003d10 then concat(fdi.SRC_ORD_DT,fdi.src_ord_num) else null end)as FM_Nov_Orders \\\n                ,count(distinct case when fy.FSCL_YR_MM\u003d11 then concat(fdi.SRC_ORD_DT,fdi.src_ord_num) else null end)as FM_Dec_Orders \\\n                ,count(distinct case when fy.FSCL_YR_MM\u003d12 then concat(fdi.SRC_ORD_DT,fdi.src_ord_num) else null end)as FM_Jan_Orders \\\n                FROM (\"+data_query+\")  as usr \\\n                    inner join us_core.VW_CONSUMER    as VC \\\n                                    on usr.edw_hshld_num \u003d vc.edw_hshld_num \\\n                    inner join us_core.FCT_DMD_ITEM  fdi \\\n                                    on vc.cnsmr_num \u003d fdi.cnsmr_num \\\n                                       and fdi.year in(2012,2013,2014,2015,2016,2017) \\\n                                       and fdi.reg_dmd_qty \u003e 0 and fdi.bus_unit_num \u003d 1 and  fdi.src_ord_Dt between \u0027{}\u0027 and \u0027{}\u0027\\\n                    inner join us_core.SKU sku    \\\n                                    on sku.SKU_NUM \u003d fdi.sku_num  \\\n                    inner join us_core.fscl_cal_dt fy  \\\n                                    on fy.CAL_DT \u003d fdi.SRC_ORD_DT  and fy.FSCL_CAL_NUM \u003d 4\\\n                group by 1\"\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\norder_time\u003dspark.sql(order_time_sql.format(independent_bdate60,independent_edate60))\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n#delay lack of data in aws\nchannel_sql\u003d\"select edw_hshld_num \\\n    ,sum(chnl_dmd) as tot_dmd     \\\n    ,sum( case when mkt_chnl \u003d \u0027Email\u0027 then chnl_dmd else 0 end )/tot_dmd as email_pct  \\\n    ,sum( case when mkt_chnl \u003d \u0027Phone\u0027 then chnl_dmd else 0 end )/tot_dmd as phone_pct \\\n    ,sum( case when mkt_chnl \u003d \u0027Direct Load\u0027 then chnl_dmd else 0 end )/tot_dmd as dload_pct \\\n    ,sum( case when mkt_chnl in (\u0027Natural Search : Non-Branded\u0027,\u0027Natural Search : brand\u0027,\u0027Natural Search : non-branded\u0027,\u0027Natural Search : Branded\u0027) then chnl_dmd else 0 end )/tot_dmd as NaturalSearch_pct   \\\n    ,sum( case when mkt_chnl in (\u0027Paid Search : Branded\u0027,\u0027Paid Search : Non-Branded\u0027) then chnl_dmd else 0 end )/tot_dmd as PaidSearch_pct \\\n    ,sum( case when mkt_chnl \u003d \u0027Affiliate\u0027 then chnl_dmd else 0 end)/tot_dmd as affl_pct \\\n    from     \\\n    (select vc.edw_hshld_num   \\\n     ,case when st.RESP_MODE_CD in (1,2,3) then \u0027Phone\u0027  \\\n      when st.resp_mode_cd \u003d 24 then \u0027Sears Marketplace\u0027   \\\n      when ( WEB_TRCK_KEY_TXT \u003d \u0027\u0027) then \u0027Direct Load\u0027    \\\n      when ( WEB_TRCK_KEY_TXT is null) then \u0027Direct Load\u0027  \\\n      when ( WEB_MKT_CHNL_TXT is NULL and WEB_TRCK_KEY_TXT like \u0027Natural Search-_-%\u0027 ) then \u0027Natural Search : Branded\u0027   \\\n      when ( WEB_MKT_CHNL_TXT \u003d \u0027Natural Search\u0027 and WEB_BRND_TXT \u003d \u0027Natural Search : brand\u0027) then \u0027Natural Search : Branded\u0027  \\\n      when ( WEB_MKT_CHNL_TXT \u003d \u0027Natural Search\u0027 and WEB_BRND_TXT \u003d \u0027Natural Search : non-branded\u0027)  then \u0027Natural Search : Non-Branded\u0027  \\\n      when ( WEB_MKT_CHNL_TXT is NULL and WEB_TRCK_KEY_TXT like \u0027usnews-_-%\u0027 ) then \u0027Email\u0027   \\\n      when ( WEB_MKT_CHNL_TXT is NULL and WEB_TRCK_KEY_TXT like \u0027CJ-_-%\u0027 ) then \u0027Affiliate\u0027  \\\n      when ( WEB_MKT_CHNL_TXT is NULL and WEB_TRCK_KEY_TXT like \u0027SI_%\u0027 ) then \u0027Paid Search : Branded\u0027  \\\n      when ( WEB_MKT_CHNL_TXT \u003d \u0027Data Feed\u0027 ) then \u0027Data Feeds\u0027  \\\n      when ( WEB_MKT_CHNL_TXT \u003d \u0027Paid Search\u0027 ) then \u0027Paid Search : Branded\u0027  \\\n      when ( WEB_MKT_CHNL_TXT \u003d \u0027Natural Search : brand\u0027 ) then \u0027Natural Search : Branded\u0027  \\\n      when ( WEB_MKT_CHNL_TXT \u003d \u0027Natural Search : non-branded\u0027 ) then \u0027Natural Search : Non-Branded\u0027  \\\n      when ( WEB_MKT_CHNL_TXT \u003d \u0027Behavioral Targeting\u0027 ) then \u0027Other\u0027   \\\n      when ( WEB_MKT_CHNL_TXT \u003d \u0027XSiteNav\u0027 ) then \u0027Other\u0027   \\\n      when ( WEB_MKT_CHNL_TXT \u003d \u0027vnr\u0027 ) then \u0027Other\u0027    \\\n      when ( WEB_MKT_CHNL_TXT \u003d \u0027Sears\u0027 ) then \u0027Other\u0027   \\\n      when ( WEB_MKT_CHNL_TXT \u003d \u0027BizRage\u0027 ) then \u0027Other\u0027  \\\n      when ( WEB_MKT_CHNL_TXT \u003d \u0027Broad\u0027 ) then \u0027Other\u0027    \\\n      when ( WEB_MKT_CHNL_TXT \u003d \u0027women-\u0027 ) then \u0027Other\u0027   \\\n      when ( WEB_MKT_CHNL_TXT \u003d \u0027yahooCA\u0027 ) then \u0027Other\u0027   \\\n      when ( WEB_MKT_CHNL_TXT \u003d \u0027cj\u0027 ) then \u0027Other\u0027       \\\n      when ( WEB_MKT_CHNL_TXT \u003d \u0027MyThings\u0027 ) then \u0027Other\u0027   \\\n      when ( WEB_MKT_CHNL_TXT is NOT NULL ) then WEB_MKT_CHNL_TXT  \\\n      when st.RESP_MODE_CD not in (5, 11, 23, 24) then cast(resp_mode_cd as char(2))  \\\n      else \u0027Other\u0027 end as mkt_chnl   \\\n      ,sum(fdi.DMD_AMT) as chnl_dmd from (\"+data_query+\")  as usr   \\\n      inner join us_core.VW_CONSUMER \t   \t\t\t\t\t  as VC   \\\n\t\t  on usr.edw_hshld_num \u003d vc.edw_hshld_num                 \\\n\t   inner join us_core.FCT_DMD_ITEM fdi                        \\\n\t\t  on vc.CUR_CNSMR_NUM \u003d fdi.CNSMR_NUM                 \\\n      inner join us_core.SALE_TRAN st                           \\\n           on fdi.INBND_CUST_EVNT_NUM \u003d st.INBND_CUST_EVNT_NUM    \\\n           and fdi.INBND_EVNT_TYP_CD \u003d st.EVNT_TYP_CD           \\\n      left join lehub.WEB_TRCK_CD WTC                          \\\n           on ST.MOST_RCNT_1_DAY_TRCK_CD \u003d wtc.web_trck_key_cd    \\\n           where fdi.SRC_ORD_DT between \u0027{}\u0027 and \u0027{}\u0027  and fdi.REG_DMD_QTY \u003e 0  and fdi.BUS_UNIT_NUM \u003d 1 \\\n           group by 1,2) A   \\\n            group by 1\"\n"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nshow functions"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nchannel\u003dspark.sql(channel_sql.format(independent_bdate60,independent_edate60))"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ntest\u003d\"select fdi.cnsmr_num \\\n ,coalesce(ch.to_cnsmr_num, fdi.cnsmr_num) as fwded_cnsmr_num \\\n ,fdi.ISS_NUM ,fdi.ISS_ID  ,fdi.ISS_YR ,fdi.ISS_comp_num  ,fdi.REG_DMD_QTY  ,fdi.src_ord_dt\n ,fdi.item_seq_num\n ,fdi.DMD_AMT\n ,fdi.BUS_UNIT_NUM\n ,fdi.SRC_ORD_NUM\n ,fdi.sku_num\n ,fdi.inbnd_cust_evnt_num\n ,fdi.inbnd_evnt_typ_cd\n from us_core.FCT_DMD_ITEM as fdi\n left join (\n select from_cnsmr_num\n ,to_cnsmr_num\n ,rank() over(partition by from_cnsmr_num order by edw_ins_ts DESC) as rank\n from us_core.cnsmr_hist\n where end_ts is null\n ) as ch\n on ch.from_cnsmr_num \u003d fdi.cnsmr_num\n and ch.rank \u003d 1\n where fdi.src_ord_dt between date_add(\u0027day\u0027, -1825, date(\u00272018-10-15\u0027)) and date(\u00272018-10-15\u0027)\"\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nweb_query\u003d\"select coalesce(hist.to_edw_hshld_num,usr.edw_hshld_num) as edw_hshld_num,round(sum((unix_timestamp(visit_end_time)-unix_timestamp(visit_start_time))/60),1) as total_duration,count(distinct visit_key) as total_visits,sum(distinct_page_num_cnt) as total_pages,max(distinct_page_num_cnt) as max_pages, \\\nround(max((unix_timestamp(visit_end_time)-unix_timestamp(visit_start_time))/60),1) as max_duration,avg(engagement_score) as score \\\nFROM (\"+data_query+\")   as usr \\\n         inner join us_core.VW_CONSUMER as VC  \\\n         on usr.edw_hshld_num \u003d vc.edw_hshld_num  \\\n         inner join us_core.web_id_cnsmr wi  \\\n         on vc.cnsmr_num\u003dwi.cnsmr_num  \\\n         left join (select from_edw_hshld_num,to_edw_hshld_num, rank() over(partition by from_edw_hshld_num order by edw_ins_ts DESC) as rank from us_core.hshld_hist where end_ts is null) as hist \\\n         on hist.from_edw_hshld_num\u003dusr.edw_hshld_num and hist.rank\u003d1\\\n         inner join core_clickstream.visits_daily vd \\\n         on wi.visid\u003dvd.visid \\\n         and vd.visit_start_time between \u0027{}\u0027 and \u0027{}\u0027\\\n         and vd.end_year\u003d\u00272018\u0027 and vd.end_month in (\u00278\u0027) \\\ngroup by 1 \\\norder by 1\""
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nweb_query\u003d\"select vc.edw_hshld_num,avg(engagement_score) as score \\\nFROM  us_core.VW_CONSUMER as VC  \\\n        inner join us_core.web_id_cnsmr wi  \\\n         on vc.cnsmr_num\u003dwi.cnsmr_num  \\\n         inner join core_clickstream.visits_daily vd \\\n         on wi.visid\u003dvd.visid \\\n         and vd.visit_start_time between \u0027{}\u0027 and \u0027{}\u0027\\\n         and vd.end_year\u003d\u00272019\u0027 and vd.end_month in (\u00277\u0027) \\\ngroup by 1\"\n"
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndata_query2\u003d\u0027\u0027\u0027select distinct c.edw_hshld_num\n                 ,concat(post_visid_high, \u0027_\u0027, post_visid_low) as post_visid_hl\n                 from  us_core.web_id_links wil\n                 left join us_core.cnsmr_hist ch\n                 on ch.from_cnsmr_num \u003d wil.cnsmr_num\n                 join us_core.vw_consumer c\n                   on c.cnsmr_num \u003d coalesce(ch.to_cnsmr_num, wil.cnsmr_num)                \n                 where method in (\u0027login\u0027, \u0027order\u0027,\u0027cics_order\u0027,\u0027email\u0027)\u0027\u0027\u0027"
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nweb_date,independent_edate60,web_year,web_mon1,web_mon2\u003d\u00272018-09-09\u0027,\u00272018-10-09\u0027,2018, 9, 10"
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n web_data_sql\u003d\u0027\u0027\u0027select count(distinct hd.EDW_HSHLD_NUM) as total\nFROM (select distinct c.edw_hshld_num\n                 ,concat(post_visid_high, \u0027_\u0027, post_visid_low) as post_visid_hl\n                 from  us_core.web_id_links wil\n                 left join us_core.cnsmr_hist ch\n                 on ch.from_cnsmr_num \u003d wil.cnsmr_num\n                 join us_core.vw_consumer c\n                   on c.cnsmr_num \u003d coalesce(ch.to_cnsmr_num, wil.cnsmr_num)                \n                 where method in (\u0027login\u0027, \u0027order\u0027,\u0027cics_order\u0027,\u0027email\u0027)) hd\ninner join core_clickstream.visits_daily vd\n on vd.post_visid_hl \u003d hd.post_visid_hl\n and vd.visit_start_time between date \u00272018-09-09\u0027 and date \u00272018-10-09\u0027\n and vd.end_year\u003d\u00272018\u0027 and vd.end_month in (\u00279\u0027,\u002710\u0027) \n\n\u0027\u0027\u0027\nweb_data\u003dspark.sql(web_data_sql)"
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nweb_data.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nweb_data.select(\u0027SCORE\u0027,\u0027TOTAL_VISITS\u0027).describe().show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndata_2x.select(\u0027edw_hshld_num\u0027).coalesce(1).write.mode(\"overwrite\").csv(\u0027s3://lerawzone/users/gyang/outfile/households_2019.csv\u0027,header\u003d\u0027true\u0027)"
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nweb\u003dspark.sql(web_query.format(\u00272018-08-01\u0027,\u00272018-08-30\u0027))"
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nweb.count()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nweb.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndata_model\u003ddepend.join(core_catalog,\u0027edw_hshld_num\u0027,how\u003d\u0027left_outer\u0027).join(dmd_order,\u0027edw_hshld_num\u0027,how\u003d\u0027left_outer\u0027).join(recency,\u0027edw_hshld_num\u0027,how\u003d\u0027inner\u0027).join(total_dmd,\u0027edw_hshld_num\u0027,how\u003d\u0027left_outer\u0027). \\\njoin(web,\u0027edw_hshld_num\u0027,how\u003d\u0027left_outer\u0027).join(total_catalog,\u0027edw_hshld_num\u0027,how\u003d\u0027left_outer\u0027).join(email_clicks,\u0027edw_hshld_num\u0027,how\u003d\u0027left_outer\u0027).join(email_opens,\u0027edw_hshld_num\u0027,how\u003d\u0027left_outer\u0027). \\\njoin(order_time,\u0027edw_hshld_num\u0027,how\u003d\u0027left_outer\u0027)"
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndata_model.write.parquet(\u0027s3://lerawzone/users/gyang/data_1x.parquet\u0027)"
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndata_1x\u003dspark.read.parquet(\u0027s3://lerawzone/users/Vijay/full_master/out/cutoff_date\u003d2020-08-07/run_date\u003d2021-07-21/data/sed_mdlgrp\u003d5\u0027)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndata_2x\u003dspark.read.parquet(\u0027s3://lerawzone/users/Tiger/Vijay/full_master/out/cutoff_date\u003d2019-08-07/run_date\u003d2021-04-14/data/sed_mdlgrp\u003d5\u0027) "
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndata_2x.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndata_2x.select(\u0027edw_hshld_num\u0027).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ncol_string\u003d[i[0] for i in data_1x.dtypes if i[1]\u003d\u003d\u0027string\u0027]\n    "
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndata_1x\u003ddata_1x.drop(col_string[0])"
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndata_1x1\u003ddata_1x[(data_1x.Total_Orders\u003e0) \u0026 (data_1x.Total_Orders.isNull()\u003d\u003dFalse) \u0026 (data_1x.Total_Orders\u003c2) ]"
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndata_1x1.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nimport pandas as pd\nimport numpy as np\nfrom pyspark.sql.functions import *"
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ncol_missing\u003dnp.array(data_1x1.columns).reshape(1,len(data_1x1.columns))[data_1x1.agg(*[(sum(col(c).isNull().cast(\u0027int\u0027))\u003e0).alias(c) for c in data_1x.columns]).toPandas().values]\n"
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfor mi in col_missing[col_missing!\u003d\u0027income\u0027]:\n    data_1x1\u003ddata_1x1.withColumn(mi,when(col(mi).isNull(),0).otherwise(col(mi)))"
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndata_1x1.agg(*[sum(col(c).isNull().cast(\u0027int\u0027)).alias(c) for c in data_1x1.columns]).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nrep_med\u003d{}\nco\u003ddata_1x1.columns[2]\nmed\u003ddata_1x1.approxQuantile(co,[0.5],0.01)\nrep_med[co]\u003dmed[0]\nrep_med\n    \n"
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndata_1x1\u003ddata_1x1.fillna(rep_med)"
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndata_1x1\u003ddata_1x1.drop(\u0027Total_Orders\u0027)"
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndata_1x1\u003ddata_1x1.drop(\u0027buyerType\u0027)"
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndata_1x1.columns"
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndata_1x1\u003ddata_1x1.withColumn(\u0027resp\u0027,when(col(\u0027dmd\u0027)\u003e0,1).otherwise(0))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndata_1x1.columns"
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndata_1x1.write.parquet(\u0027s3://lerawzone/users/gyang/data_clean.parquet\u0027)"
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndata_clean\u003dspark.read.parquet(\u0027s3://lerawzone/users/gyang/data_clean.parquet\u0027)"
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nimport pyspark.ml.feature as ft\nimport pyspark.ml.classification as cl\nimport pyspark.ml.evaluation as ev\nfrom pyspark.ml import Pipeline\nimport pyspark.ml.tuning as tune\nimport itertools"
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfeatures\u003ddata_1x1.columns[2:-1]\nfeatureCreator\u003dft.VectorAssembler(inputCols\u003dfeatures,outputCol\u003d\u0027features\u0027)"
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ntrain_1x1,test_1x1\u003ddata_1x1.randomSplit([0.7,0.3],seed\u003d123)\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nrf\u003dcl.GBTClassifier(maxDepth\u003d4,labelCol\u003d\u0027resp\u0027,maxIter\u003d40)\npipeline\u003dPipeline(stages\u003d(featureCreator,rf))\nmodel_1x1\u003dpipeline.fit(train_1x1)"
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\npre_train\u003dmodel_1x1.transform(train_1x1)\npre_test\u003dmodel_1x1.transform(test_1x1)"
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nevaluator\u003dev.BinaryClassificationEvaluator(rawPredictionCol\u003d\u0027probability\u0027,labelCol\u003d\u0027resp\u0027)\nprint(evaluator.evaluate(pre_train,{evaluator.metricName:\u0027areaUnderROC\u0027}))\nprint(evaluator.evaluate(pre_test,{evaluator.metricName:\u0027areaUnderROC\u0027}))"
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndata_1x1\u003ddata_1x1.repartition(100)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nrf1\u003dcl.RandomForestClassifier(labelCol\u003d\u0027resp\u0027)\n#grid\u003dtune.ParamGridBuilder().addGrid(rf1.maxDepth,[5,6,7,8]).addGrid(rf1.numTrees,[100,110,111]).build()\n#cv\u003dtune.CrossValidator(estimator\u003drf1,estimatorParamMaps\u003dgrid,evaluator\u003devaluator,numFolds\u003d5)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ncv_model_1x1\u003dcv.fit(featureCreator.transform(data_clean))"
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nparams \u003d [{p.name: v for p, v in m.items()} for m in cv_model_1x1.getEstimatorParamMaps()]\nfor ps,metric in zip(params,cv_model_1x1.avgMetrics):\n    ps[cv_model_1x1.getEvaluator().getMetricName()]\u003dmetric\nres\u003dpd.DataFrame(params).iloc[:,[1,2,0]]\nres\n"
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndata1,data2,data3,data4,data5\u003ddata_1x1.randomSplit([0.2,0.2,0.2,0.2,0.2],seed\u003d12)"
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nimport itertools\nx\u003dlist(range(1,6))\ntotal\u003dlist(itertools.combinations(x,4))"
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nlen(total)"
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndata_list\u003d[data1,data2,data3,data4,data5]"
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndata_train\u003d[]\nfor i in range(len(total)):\n    data_train.append(spark.createDataFrame(sc.emptyRDD(),data_list[i].schema))"
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndata_test_index\u003d[]"
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfull_total\u003d[1,2,3,4,5]\n"
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nfor i in range(len(total)):\n    for j in total[i]:\n        data_train[i]\u003ddata_train[i].union(eval(\u0027data\u0027+str(j)))\n    data_test_index.append(list(set(full_total).difference(set(total[i])))[0])\n        \n"
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfor i in range(len(total)):\n    data_train[i]\u003ddata_train[i].repartition(100)"
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndata_test_index"
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndata_trai\n"
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfeatures"
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nresult\u003dnp.zeros([60,5])\nj\u003d0\nfor i in range(len(data_train)):\n    for maxdep in range(5,9):\n        for tree in range(100,130,10):\n            rf2\u003dcl.RandomForestClassifier(labelCol\u003d\u0027resp\u0027,maxDepth\u003dmaxdep,numTrees\u003dtree)\n            cv_model_c\u003drf2.fit(featureCreator.transform(train_1x1))\n            pre_train\u003dcv_model_c.transform(featureCreator.transform(train_1x1))\n            pre_test\u003dcv_model_c.transform(featureCreator.transform(eval(\u0027data\u0027+str(data_test_index[i]))))\n            result[j,0]\u003di\n            result[j,1]\u003dmaxdep\n            result[j,2]\u003dtree\n            result[j,3]\u003devaluator.evaluate(pre_train,{evaluator.metricName:\u0027areaUnderROC\u0027})\n            result[j,4]\u003devaluator.evaluate(pre_test,{evaluator.metricName:\u0027areaUnderROC\u0027})\n            j+\u003d1\n    \n\n        \n        "
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nresult.shape"
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nres1\u003dspark.createDataFrame(pd.DataFrame(result,columns\u003d[\u0027Num_Folds\u0027,\u0027Depth\u0027,\u0027NumTrees\u0027,\u0027Train_Score\u0027,\u0027Test_score\u0027]))\nres1"
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nres1.show(60)"
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nres1.groupby([\u0027Depth\u0027,\u0027NumTrees\u0027]).agg({\u0027Train_Score\u0027:\u0027mean\u0027,\u0027Test_Score\u0027:\u0027mean\u0027}).orderBy([\u0027Depth\u0027,\u0027NumTrees\u0027]).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nbest\u003dcl.RandomForestClassifier(labelCol\u003d\u0027resp\u0027,maxDepth\u003d5,numTrees\u003d110)\npipeline\u003dPipeline(stages\u003d[featureCreator,best])\nbest_model\u003dpipeline.fit(train_1x1)\ntrain_pre\u003dbest_model.transform(train_1x1)\ntest_pre\u003dbest_model.transform(test_1x1)"
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nprint(evaluator.evaluate(train_pre,{evaluator.metricName:\u0027areaUnderROC\u0027}))\nprint(evaluator.evaluate(test_pre,{evaluator.metricName:\u0027areaUnderROC\u0027}))"
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nresult2\u003dnp.zeros([60,5])\nj\u003d0\nfor i in range(5):\n    for maxdep in range(3,7):\n        for tree in range(20,50,10):\n            rf3\u003dcl.GBTClassifier(labelCol\u003d\u0027resp\u0027,maxDepth\u003dmaxdep,maxIter\u003dtree)\n            cv_model_g\u003drf3.fit(featureCreator.transform(data_train[i]))\n            pre_train_g\u003dcv_model_g.transform(featureCreator.transform(data_train[i]))\n            pre_test_g\u003dcv_model_g.transform(featureCreator.transform(eval(\u0027data\u0027+str(data_test_index[i]))))\n            result2[j,0]\u003di\n            result2[j,1]\u003dmaxdep\n            result2[j,2]\u003dtree\n            result2[j,3]\u003devaluator.evaluate(pre_train_g,{evaluator.metricName:\u0027areaUnderROC\u0027})\n            result2[j,4]\u003devaluator.evaluate(pre_test_g,{evaluator.metricName:\u0027areaUnderROC\u0027})\n            j+\u003d1\n\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nres2\u003dspark.createDataFrame(pd.DataFrame(result2,columns\u003d[\u0027Num_Folds\u0027,\u0027Depth\u0027,\u0027NumTrees\u0027,\u0027Train_Score\u0027,\u0027Test_score\u0027]))\nres2.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nres2.groupby([\u0027Depth\u0027,\u0027NumTrees\u0027]).agg(*[mean(col(co)).alias(\u0027avg\u0027+co) for co in res2.columns[3:]]).orderBy([\u0027Depth\u0027,\u0027NumTrees\u0027]).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pysparke\nempty_data\u003dspark.createDataFrame(sc.emptyRDD(),data1.schema)"
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndata1.distinct().count()"
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndata1.dropDuplicates().count()"
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndata2.distinct().count()"
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndata2.count()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndata1.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfor i in total[0]:\n    train1_data\u003deval(\u0027data\u0027+str)"
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nlist([1,2,3,4,5])list([1,2,3,4])"
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nparams"
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ncv_model_1x1.getEvaluator().getMetricName()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndatas.agg({\u0027visits\u0027:\u0027sum\u0027}).alias(\u0027Total_Visits\u0027).show()\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql.types import *\nimport pyspark.sql.types as typ\ndatas\u003ddatas.withColumn(\u0027age\u0027,datas.age.cast(typ.FloatType()))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndatas.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql.functions import *\ndatas.where(isnan(datas.age)).show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndatas[\u0027visits\u0027,\u0027animal\u0027].groupby(\u0027animal\u0027).mean().show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndatas.sort([\u0027age\u0027,\u0027animal\u0027],ascending\u003d[True,False]).show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndatas\u003ddatas.withColumn(\u0027priority\u0027,when(datas.priority\u003d\u003d\u0027yes\u0027,True).otherwise(False))\ndatas.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndatas.groupby(\u0027animal\u0027).pivot(\u0027visits\u0027).sum(\u0027age\u0027).show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndfs \u003d spark.createDataFrame([(1, 2, 2, 3, 4, 5, 5, 5, 6, 7, 7)],[\u0027A\u0027])\ndfs.show()\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndatas[datas.age.isNull()].show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf_miss \u003d spark.createDataFrame([\n(1, 143.5, 5.6, 28, \u0027M\u0027, 100000),\n(2, 167.2, 5.4, 45, \u0027M\u0027, None),\n(3, None , 5.2, None, None, None),\n(4, 144.5, 5.9, 33, \u0027M\u0027, None),\n(5, 133.2, 5.7, 54, \u0027F\u0027, None),\n(6, 124.1, 5.2, None, \u0027F\u0027, None),\n(7, 129.2, 5.3, 42, \u0027M\u0027, 76000),\n], [\u0027id\u0027, \u0027weight\u0027, \u0027height\u0027, \u0027age\u0027, \u0027gender\u0027, \u0027income\u0027])\ndf_miss.show(20)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf_miss[df_miss.age.isNull()].show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfile_names\u003d[]\nfor i in range(1,11):\n    file_names.append(\u0027s3://lerawzone/core/sas/score_012_sese_d060722_pt.csv.gz/score_012_sese_d060722_pt\u0027\n                      +str(i)+\u0027.csv.gz\u0027)\nfile_names"
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nsese_012_h\u003dspark.read.csv(\u0027s3://lerawzone/core/sas/score_012_sese_d060722_pt.csv.gz/score_012_sese_d060722_pt0.csv.gz\u0027,header\u003dTrue, inferSchema\u003dTrue)\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nlen(sese_012_h.columns)"
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nsese_1360_h\u003dspark.read.csv(\u0027s3://lerawzone/core/sas/score_1360_sese_d060722_pt.csv.gz/score_1360_sese_d060722_pt0.csv.gz\u0027,header\u003dTrue, inferSchema\u003dTrue)"
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nlen(sese_1360_h.columns)"
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfile_1360_names\u003d[]\nfor i in range(1,11):\n    file_1360_names.append(\u0027s3://lerawzone/core/sas/score_1360_sese_d060722_pt.csv.gz/score_1360_sese_d060722_pt\u0027+str(i)+\u0027.csv.gz\u0027)\nfile_1360_names"
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nsese_012_data\u003dspark.read.csv(file_names,header\u003dFalse,inferSchema\u003dTrue)"
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nsese_1360_data\u003dspark.read.csv(file_1360_names,header\u003dFalse,inferSchema\u003dTrue)"
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nsese_012_data\u003dsese_012_data.toDF(*sese_012_h.columns)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nsese_1360_data\u003dsese_1360_data.toDF(*sese_1360_h.columns)"
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nsese_012_data.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nsese_1360_data.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nsese_012_data.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nsese_1360_data.printSchema()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfor old_name,new_name in zip(sese_012_data.columns,sese_012_h.columns):\n    sese_012_data\u003dsese_012_data.withColumnRenamed(old_name,new_name)\n    \n    \n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ntotal_012_var\u003d[\u0027CASUAL_DMD_0_60\u0027,\n \u0027CASUAL_itm_0_60\u0027,\n \u0027CPCO_060\u0027,\n \u0027CP_AOV0_12\u0027,\n \u0027CP_AOV0_24\u0027,\n \u0027CP_AOV0_36\u0027,\n \u0027CP_AOV0_48\u0027,\n \u0027CP_AOV0_60\u0027,\n \u0027CP_AOV13_60\u0027,\n \u0027CP_DMD0_24\u0027,\n \u0027CP_DMD0_36\u0027,\n \u0027CP_DMD0_48\u0027,\n \u0027CP_DMD0_60\u0027,\n \u0027CP_DMD_JUNJULAUG\u0027,\n \u0027CP_ITM0_24\u0027,\n \u0027CP_ITM0_36\u0027,\n \u0027CP_ITM0_48\u0027,\n \u0027CP_ITM0_60\u0027,\n \u0027CP_LPA\u0027,\n \u0027CP_LQUNT\u0027,\n \u0027CP_ORD0_24\u0027,\n \u0027CP_ORD0_36\u0027,\n \u0027CP_ORD0_48\u0027,\n \u0027CP_ORD_MARAPRMAY\u0027,\n \u0027IN_LPA\u0027,\n \u0027IN_ORD0_24\u0027,\n \u0027IN_ORD0_36\u0027,\n \u0027IN_ORD13_60\u0027,\n \u0027KD_BOY_WM_ITM060\u0027,\n \u0027KD_FT_WM_ITM060\u0027,\n \u0027KD_FT_WM_ITM1360\u0027,\n \u0027KD_GRL_WM_ITM012\u0027,\n \u0027KD_GRL_WM_ITM060\u0027,\n \u0027KD_LUG_WM_ITM060\u0027,\n \u0027KD_NEW_WM_ITM060\u0027,\n \u0027KD_OUT_WM_ITM060\u0027,\n \u0027KD_SLEEP_WM_ITM060\u0027,\n \u0027KD_UNI_WM_ITM060\u0027,\n \u0027LITKD_WM_ITM060\u0027,\n \u0027MENS_AOV0_12\u0027,\n \u0027MENS_AOV0_24\u0027,\n \u0027MENS_AOV0_36\u0027,\n \u0027MENS_AOV0_48\u0027,\n \u0027MENS_AOV0_60\u0027,\n \u0027MENS_AOV13_60\u0027,\n \u0027MENS_CASUAL_DMD0_60\u0027,\n \u0027MN_DMD0_24\u0027,\n \u0027MN_DMD0_36\u0027,\n \u0027MN_DMD0_48\u0027,\n \u0027MN_DMD0_60\u0027,\n \u0027MN_ITM0_60\u0027,\n \u0027MN_WM_ITM0_12\u0027,\n \u0027MN_WM_ITM0_24\u0027,\n \u0027MN_WM_ITM0_36\u0027,\n \u0027MN_WM_ITM0_48\u0027,\n \u0027MN_WM_ITM0_60\u0027,\n \u0027MN_WM_ITM13_60\u0027,\n \u0027SQRT_CP_ORD0_12\u0027,\n \u0027SQRT_CP_ORD0_24\u0027,\n \u0027SQRT_CP_ORD0_36\u0027,\n \u0027SQRT_CP_ORD0_48\u0027,\n \u0027SQRT_CP_ORD0_60\u0027,\n \u0027WM_ITM0_24\u0027,\n \u0027WM_ITM0_36\u0027,\n \u0027WM_ITM0_48\u0027,\n \u0027WM_ITM0_60\u0027,\n \u0027WM_ORD0_24\u0027,\n \u0027WM_ORD0_48\u0027,\n \u0027WM_ORD0_60\u0027,\n \u0027WOMENS_AOV0_12\u0027,\n \u0027WOMENS_AOV0_24\u0027,\n \u0027WOMENS_AOV0_36\u0027,\n \u0027WOMENS_AOV0_48\u0027,\n \u0027WOMENS_AOV0_60\u0027,\n \u0027WOMENS_AOV13_60\u0027,\n \u0027WOMENS_CASUAL_ITM0_60\u0027,\n \u0027WOMENS_CASUAL_ITM_0_36\u0027,\n \u0027Wn_Mn_012\u0027,\n \u0027Wn_Mn_1360\u0027]\nlen(total_012_var)"
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfinal_total_012_var\u003d[sese_012_data.columns[0]]+total_012_var\nfinal_total_012_var\n"
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfinal_total_1360_var\u003d[\u0027EDW_HSHLD_NUM\u0027,\n\u0027CASUAL_DMD_0_60\u0027,\n \u0027CASUAL_DMD_13_60\u0027,\n \u0027CASUAL_itm_0_60\u0027,\n \u0027CASUAL_itm_13_60\u0027,\n \u0027CPCO_1360\u0027,\n \u0027CP_AOV0_36\u0027,\n \u0027CP_AOV0_48\u0027,\n \u0027CP_AOV0_60\u0027,\n \u0027CP_AOV13_60\u0027,\n \u0027CP_CI24\u0027,\n \u0027CP_CO24\u0027,\n \u0027CP_DMD0_36\u0027,\n \u0027CP_DMD13_60\u0027,\n \u0027CP_ITM0_24\u0027,\n \u0027CP_ITM0_36\u0027,\n \u0027CP_ITM0_48\u0027,\n \u0027CP_ITM0_60\u0027,\n \u0027CP_LPA\u0027,\n \u0027CP_ORD0_36\u0027,\n \u0027CP_ORD0_48\u0027,\n \u0027EMAIL_SUB_CD\u0027,\n \u0027INET_DMD_0_36\u0027,\n \u0027IN_CD24\u0027,\n \u0027IN_DMD0_36\u0027,\n \u0027IN_DMD0_48\u0027,\n \u0027IN_DMD0_60\u0027,\n \u0027IN_ORD0_36\u0027,\n \u0027IN_ORD0_48\u0027,\n \u0027IN_ORD13_60\u0027,\n \u0027KD_FT_WM_ITM060\u0027,\n \u0027KD_GRL_WM_ITM060\u0027,\n \u0027KD_GRL_WM_ITM1360\u0027,\n \u0027KD_LUG_WM_ITM060\u0027,\n \u0027KD_LUG_WM_ITM1360\u0027,\n \u0027KD_NEW_WM_ITM060\u0027,\n \u0027KD_OUT_WM_ITM060\u0027,\n \u0027KD_SLEEP_WM_ITM060\u0027,\n \u0027KD_UNI_WM_ITM060\u0027,\n \u0027LITKD_WM_ITM060\u0027,\n \u0027LITKD_WM_ITM1360\u0027,\n \u0027MENS_AOV0_24\u0027,\n \u0027MENS_AOV0_36\u0027,\n \u0027MENS_AOV0_48\u0027,\n \u0027MENS_AOV0_60\u0027,\n \u0027MENS_AOV13_60\u0027,\n \u0027MENS_BOTTOMS_DMD_13_60\u0027,\n \u0027MENS_CASUAL_DMD_13_60\u0027,\n \u0027MENS_TAILORED_DMD_0_36\u0027,\n \u0027MENS_TAILORED_ITM0_60\u0027,\n \u0027MENS_TOPS_DMD_13_60\u0027,\n \u0027MN_CD24\u0027,\n \u0027MN_CI24\u0027,\n \u0027MN_DMD0_24\u0027,\n \u0027MN_DMD0_36\u0027,\n \u0027MN_DMD0_48\u0027,\n \u0027MN_DMD0_60\u0027,\n \u0027MN_DMD13_60\u0027,\n \u0027MN_ITM0_48\u0027,\n \u0027MN_ITM0_60\u0027,\n \u0027MN_ITM13_60\u0027,\n \u0027MN_LPA\u0027,\n \u0027MN_WM_ITM0_36\u0027,\n \u0027MN_WM_ITM0_48\u0027,\n \u0027MN_WM_ITM0_60\u0027,\n \u0027MN_WM_ITM13_60\u0027,\n \u0027OFF_MT_PGS_012\u0027,\n \u0027OFF_SWIM_PGS_012\u0027,\n \u0027OFF_TOT_24\u0027,\n \u0027OFF_TOT_LE4M_24\u0027,\n \u0027OFF_WT_PGS_012\u0027,\n \u0027SQRT_CP_ORD0_48\u0027,\n \u0027TOT_KD_WM_ITM1360\u0027,\n \u0027WM_DMD13_60\u0027,\n \u0027WM_ITM0_48\u0027,\n \u0027WM_ORD0_60\u0027,\n \u0027WOMENS_AOV0_48\u0027,\n \u0027WOMENS_AOV0_60\u0027,\n \u0027WOMENS_AOV13_60\u0027,\n \u0027WOMENS_CASUAL_ITM_13_60\u0027,\n \u0027WOMENS_SWIMWEAR_ITM_13_60\u0027,\n \u0027Wn_Mn_1360\u0027,\n \u0027avg_engage_score\u0027,\n \u0027max_pages_visit\u0027,\n \u0027max_vis_duration\u0027,\n \u0027total_duration\u0027,\n \u0027total_pages\u0027]\nfinal_total_1360_var"
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nsese_012_score_data\u003dsese_012_data.select(*final_total_012_var)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nsese_1360_score_data\u003dsese_1360_data.select(*final_total_1360_var)"
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nlen(sese_012_score_data.columns)"
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nsese_1360_score_data.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nsese_012_score_data.write.parquet(\u0027s3://lerawzone/users/gyang/outfile/sese_012_score_data_d060722.parquet\u0027)"
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nsese_1360_score_data.write.parquet(\u0027s3://lerawzone/users/gyang/outfile/sese_1360_score_data_d060722.parquet\u0027)"
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nbirths\u003dspark.read.csv(\u0027s3://lerawzone/users/gyang/births_transformed.csv.gz\u0027,header\u003dTrue)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nsese_012_data.select(*final_total_012_var).coalesce(1).write.mode(\u0027overwrite\u0027).csv(\u0027s3://lerawzone/users/gyang/outfile/births_test3.csv\u0027,header\u003d\u0027true\u0027)"
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nbirths.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nvar x\u003dList(1,2,3,4)"
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nwese_score.select(\u0027ab_score\u0027).describe().show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    }
  ]
}