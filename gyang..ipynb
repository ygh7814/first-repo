{
  "metadata": {
    "name": "gyang",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%spark.pyspark\ntext_data \u003d spark.createDataFrame([\n[\u0027\u0027\u0027Machine learning can be applied to a wide variety\nof data types, such as vectors, text, images, and\nstructured data. This API adopts the DataFrame from\nSpark SQL in order to support a variety of data\ntypes.\u0027\u0027\u0027],\n[\u0027\u0027\u0027DataFrame supports many basic and structured types; \nsee the Spark SQL datatype reference for a list of supported types. \nIn addition to the types listed in the Spark SQL guide, DataFrame can use ML Vector types.\u0027\u0027\u0027],\n[\u0027\u0027\u0027A DataFrame can be created either implicitly or explicitly from a regular RDD. \nSee the code examples below and the Spark SQL programming guide for examples.\u0027\u0027\u0027],\n\n[\u0027\u0027\u0027Columns in a DataFrame are named. The code examples\nbelow use names such as \"text,\" \"features,\" and\n\"label.\"\u0027\u0027\u0027]\n], [\u0027input\u0027])\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ntext_data.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nimport pandas as pd\nimport numpy as np\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ntext_data.coalesce(1).write.mode(\u0027overwrite\u0027).csv(\u0027s3://lerawzone/users/gyang/outfile/text_data.csv\u0027,header\u003d\u0027true\u0027)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nimport pyspark.sql.types as typ\nlabels \u003d [\n(\u0027INFANT_ALIVE_AT_REPORT\u0027, typ.IntegerType()),\n(\u0027BIRTH_PLACE\u0027, typ.StringType()),\n(\u0027MOTHER_AGE_YEARS\u0027, typ.IntegerType()),\n(\u0027FATHER_COMBINED_AGE\u0027, typ.IntegerType()),\n(\u0027CIG_BEFORE\u0027, typ.IntegerType()),\n(\u0027CIG_1_TRI\u0027, typ.IntegerType()),\n(\u0027CIG_2_TRI\u0027, typ.IntegerType()),\n(\u0027CIG_3_TRI\u0027, typ.IntegerType()),\n(\u0027MOTHER_HEIGHT_IN\u0027, typ.IntegerType()),\n(\u0027MOTHER_PRE_WEIGHT\u0027, typ.IntegerType()),\n(\u0027MOTHER_DELIVERY_WEIGHT\u0027, typ.IntegerType()),\n(\u0027MOTHER_WEIGHT_GAIN\u0027, typ.IntegerType()),\n(\u0027DIABETES_PRE\u0027, typ.IntegerType()),\n(\u0027DIABETES_GEST\u0027, typ.IntegerType()),\n(\u0027HYP_TENS_PRE\u0027, typ.IntegerType()),\n(\u0027HYP_TENS_GEST\u0027, typ.IntegerType()),\n(\u0027PREV_BIRTH_PRETERM\u0027, typ.IntegerType())]\nschema\u003dtyp.StructType([typ.StructField(e[0],e[1],False) for e in labels])\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nimport xgboost\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql.types import *\n"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nschema\u003dStructType([StructField(\u0027INFANT_ALIVE_AT_REPORT\u0027,LongType(),True),\n                  StructField(\u0027BIRTH_PLACE\u0027,LongType(),True)])\n"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nbirths\u003dspark.read.csv(\u0027s3://lerawzone/users/gyang/births_transformed.csv.gz\u0027,header\u003dTrue)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nemail\u003dspark.read.csv(\u0027s3://lerawzone/users/gyang/outfile/email_address.csv\u0027,header\u003dTrue)"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nbirths.select(*births.columns[:5]).coalesce(1).write.mode(\u0027overwrite\u0027).csv(\u0027s3://lerawzone/users/gyang/outfile/births_test2.csv\u0027,header\u003d\u0027true\u0027)"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nparquet\u003dspark.read.parquet(\u0027s3://lerefinedzone/core/paid_search/crealytics_order_feed/create_dt\u003d2018-06-14/*\u0027)"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nbirths1\u003dbirths.select(*births.columns[:5])\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nbirths1.show(10)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nbirths.\n"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nbirths.describe().show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql.functions import *\nimport pyspark.ml.feature as ft\nfrom pyspark.ml.feature import Binarizer\nimport pyspark.ml.classification as cl\nfrom pyspark.ml import Pipeline\nimport pyspark.ml.evaluation as ev\nimport pyspark.ml.tuning as tune\nfrom pyspark.mllib.evaluation import MulticlassMetrics\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nbirths.count()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfor column in births.columns:\n    births\u003dbirths.withColumn(column,births[column].cast(\u0027integer\u0027))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nbirths.printSchema()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfeatures\u003dbirths.columns[1:]\nfeatures\n"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nbirths.select(*(sum(col(c).isNull().cast(\u0027integer\u0027)).alias(c) for c in births.columns)).show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfor i in births.columns:\n    print(births.select(i).distinct().count())\n"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nbirths.distinct().count()"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nbirths.createOrReplaceTempView(\u0027births_sql\u0027)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\u0027select count(distinct *) from births_sql\u0027).show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\u0027select count(*) from births_sql\u0027).show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nbirths2\u003dbirths.dropDuplicates()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql.functions import *"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf \u003d sqlContext.createDataFrame([\n    (1, \"a\"),\n    (2, \"b\"),\n    (3, \"c\"),\n], [\"ID\", \"Text\"])\n\ncategories \u003d df.select(\"Text\").distinct().rdd.flatMap(lambda x: x).collect()"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nexprs \u003d [when(col(\"Text\") \u003d\u003d category, 1).otherwise(0).alias(category)\n         for category in categories]\ntype(exprs)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf\u003ddf.select(\u0027ID\u0027,*exprs)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nencoder\u003dft.OneHotEncoder(inputCol\u003d\u0027BIRTH_PLACE\u0027,outputCol\u003d\u0027BIRTH_PLACE_VEC\u0027)\ntype(encoder)"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nencoder.getOutputCol()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nfeaturesCreator\u003dft.VectorAssembler(inputCols\u003d[col for col in features],outputCol\u003d\u0027features\u0027)\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nbirths2\u003dfeaturesCreator.transform(births)"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nregP\u003drange(0.01,0.1,0.01)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nlogistic\u003dcl.LogisticRegression(maxIter\u003d50,regParam\u003d0.0,labelCol\u003d\u0027INFANT_ALIVE_AT_REPORT\u0027)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\npipeline\u003dPipeline(stages\u003d[featuresCreator,logistic])\n##births_train,births_test\u003dbirths.randomSplit([0.7,0.3],seed\u003d666)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nbirths_train.show(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nbirths2_train,births2_test\u003dbirths2.randomSplit([0.7,0.3],seed\u003d666)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nbirths_train,births_test\u003dbirths.randomSplit([0.7,0.3],seed\u003d666)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nmodel2\u003dpipeline.fit(births2_train)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ntrain2\u003dmodel2.transform(births2_train)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ntest2\u003dmodel2.transform(births2_test)\ntest2.take(1)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ntest2.select(\u0027probability\u0027).show(10)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nmodel\u003dpipeline.fit(births_train)\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ntest_model\u003dmodel.transform(births_test)\ntest_model.take(1)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ntest_model.select(\u0027probability\u0027).show(10)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nevaluator\u003dev.BinaryClassificationEvaluator(rawPredictionCol\u003d\u0027probability\u0027,labelCol\u003dbirths_train.columns[0])\ntrain_model\u003dmodel.transform(births_train)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nevaluator2\u003dev.BinaryClassificationEvaluator(rawPredictionCol\u003d\u0027probability\u0027,labelCol\u003dbirths2_train.columns[0])\n"
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nprint(evaluator2.evaluate(train2,{evaluator2.metricName:\u0027areaUnderROC\u0027}))\nprint(evaluator2.evaluate(test2,{evaluator2.metricName:\u0027areaUnderROC\u0027}))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nprint(evaluator.evaluate(train_model,{evaluator.metricName:\u0027areaUnderROC\u0027}))\nprint(evaluator.evaluate(test_model,{evaluator.metricName:\u0027areaUnderROC\u0027}))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nimport numpy as np\nimport pandas as pd\n"
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nnp.zeros((6,3))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nresult\u003dnp.zeros((6,3))\nfor i,depth in enumerate(range(5,11)):\n    rf_classifier\u003dcl.RandomForestClassifier(maxDepth\u003ddepth,labelCol\u003d\u0027INFANT_ALIVE_AT_REPORT\u0027,numTrees\u003d100)\n    rf_pip\u003dPipeline(stages\u003d[featuresCreator,rf_classifier])\n    rf_model\u003drf_pip.fit(births_train)\n    rf_train\u003drf_model.transform(births_train)\n    rf_test\u003drf_model.transform(births_test)\n    rf_eval\u003dev.BinaryClassificationEvaluator(rawPredictionCol\u003d\u0027probability\u0027,labelCol\u003dbirths.columns[0])\n    result[i,0]\u003ddepth\n    result[i,1]\u003drf_eval.evaluate(rf_train,{rf_eval.metricName:\u0027areaUnderROC\u0027})\n    result[i,2]\u003drf_eval.evaluate(rf_test,{rf_eval.metricName:\u0027areaUnderROC\u0027})\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nresult"
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nrf_eval\u003dev.BinaryClassificationEvaluator(rawPredictionCol\u003d\u0027probability\u0027,labelCol\u003dbirths.columns[0])\nprint(rf_eval.evaluate(rf_train,{rf_eval.metricName:\u0027areaUnderROC\u0027}))\nprint(rf_eval.evaluate(rf_test,{rf_eval.metricName:\u0027areaUnderROC\u0027}))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nresult1\u003dnp.zeros((6,3))\nfor i,depth in enumerate(range(5,11)):\n    GB_classifier\u003dcl.GBTClassifier(labelCol\u003d\u0027INFANT_ALIVE_AT_REPORT\u0027,maxDepth\u003ddepth)\n    GB_pip\u003dPipeline(stages\u003d[featuresCreator,GB_classifier])\n    GB_model\u003dGB_pip.fit(births_train)\n    GB_train\u003dGB_model.transform(births_train)\n    GB_test\u003dGB_model.transform(births_test)\n    GB_eval\u003dev.BinaryClassificationEvaluator(rawPredictionCol\u003d\u0027probability\u0027,labelCol\u003dbirths.columns[0])\n    result1[i,0]\u003ddepth\n    result1[i,1]\u003dGB_eval.evaluate(GB_train,{GB_eval.metricName:\u0027areaUnderROC\u0027})\n    result1[i,2]\u003dGB_eval.evaluate(GB_test,{GB_eval.metricName:\u0027areaUnderROC\u0027})\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nresult1\n"
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nGB_eval\u003dev.BinaryClassificationEvaluator(rawPredictionCol\u003d\u0027probability\u0027,labelCol\u003dbirths.columns[0])\nprint(GB_eval.evaluate(GB_train,{GB_eval.metricName:\u0027areaUnderROC\u0027}))\nprint(GB_eval.evaluate(GB_test,{GB_eval.metricName:\u0027areaUnderROC\u0027}))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ngrid\u003dtune.ParamGridBuilder().addGrid(logistic.maxIter,[2,10,50]).addGrid(logistic.regParam,[0.01,0.05,0.3]).build()\ncv\u003dtune.CrossValidator(estimator\u003dlogistic,estimatorParamMaps\u003dgrid,evaluator\u003devaluator)\npipeline\u003dPipeline(stages\u003d[encoder,featuresCreator])\ndata_transformer\u003dpipeline.fit(births_train)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ncvModel\u003dcv.fit(data_transformer.transform(births_train))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndata_test\u003ddata_transformer.transform(births_test)\ndata_train\u003ddata_transformer.transform(births_train)\nresults_train\u003dcvModel.transform(data_train)\nresults_test\u003dcvModel.transform(data_test)\nprint(evaluator.evaluate(results_train,{evaluator.metricName:\u0027areaUnderROC\u0027}))\nprint(evaluator.evaluate(results_test,{evaluator.metricName:\u0027areaUnderROC\u0027}))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ncvModel\u003dcv.fit(data_transformer.transform(births_train))\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nsvc_classifier\u003dcl.LinearSVC(labelCol\u003d\u0027INFANT_ALIVE_AT_REPORT\u0027)\npip\u003dPipeline(stages\u003d[featuresCreator,svc_classifier])\nsvc_model\u003dpip.fit(births_train)\ntrain_prob\u003dsvc_model.transform(births_train)\ntest_prob\u003dsvc_model.transform(births_test)\nsvc_eval\u003dev.BinaryClassificationEvaluator(labelCol\u003d\u0027INFANT_ALIVE_AT_REPORT\u0027)\nprint(svc_eval.evaluate(train_prob,{svc_eval.metricName:\u0027areaUnderROC\u0027}))\nprint(svc_eval.evaluate(test_prob,{svc_eval.metricName:\u0027areaUnderROC\u0027}))\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nweather\u003dspark.read.csv(\u0027s3://lerawzone/users/gyang/daily_weather.csv\u0027,header\u003dTrue)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nprint(weather.count())\nweather.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nweather\u003dweather.drop(\u0027number\u0027)\nweather.columns\n"
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfor cols in weather.columns:\n    weather\u003dweather.withColumn(cols,col(cols).cast(\u0027float\u0027))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nweather\u003dweather.dropna()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nweather.show(6)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nweather\u003dweather.withColumn(\u0027relative_humidity_3pm\u0027,when(col(\u0027relative_humidity_3pm\u0027)\u003c24.99999,0).otherwise(1))\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nweather.printSchema()\nprint(weather.count())\n"
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfeatures\u003dweather.columns[:-1]\nfeaturesAss\u003dft.VectorAssembler(inputCols\u003dfeatures,outputCol\u003d\u0027features\u0027)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ntrain_weather,test_weather\u003dweather.randomSplit([0.7,0.3],seed\u003d13234)\nprint(train_weather.count(),test_weather.count())\n"
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ntrain_weather\u003dfeaturesAss.transform(train_weather)\ntest_weather\u003dfeaturesAss.transform(test_weather)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndt\u003dcl.DecisionTreeClassifier(featuresCol\u003d\u0027features\u0027,labelCol\u003d\u0027relative_humidity_3pm\u0027,maxDepth\u003d3,minInstancesPerNode\u003d5,impurity\u003d\u0027gini\u0027)\nmodel\u003ddt.fit(train_weather)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ntrain_pred\u003dmodel.transform(train_weather)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ntrain_pred.select([\u0027probability\u0027]).show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ntest_pred\u003dmodel.transform(test_weather)\ntest_pred.select(\u0027relative_humidity_3pm\u0027,\u0027prediction\u0027).show(10)"
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nlabels\n"
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nevaluator2\u003dev.BinaryClassificationEvaluator(rawPredictionCol\u003d\u0027prediction\u0027,labelCol\u003d\u0027relative_humidity_3pm\u0027)"
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\npredictions_mat\u003dtrain_pred.select(\u0027prediction\u0027,\u0027relative_humidity_3pm\u0027)\npredictions_mat.rdd.take(10)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nprint(evaluator2.evaluate(train_pred,{evaluator2.metricName:\u0027accuracy\u0027}))\nprint(evaluator2.evaluate(test_pred,{evaluator2.metricName:\u0027accuracy\u0027}))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\npredictions_mat\u003dtrain_pred.select(\u0027prediction\u0027,\u0027relative_humidity_3pm\u0027)\nmetrics\u003dMulticlassMetrics(predictions_mat.rdd.map(tuple))\nmetrics.confusionMatrix().toArray()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nevaluator3\u003dev.MulticlassClassificationEvaluator(predictionCol\u003d\u0027prediction\u0027,labelCol\u003d\u0027relative_humidity_3pm\u0027)\nprint(evaluator3.evaluate(train_pred,{evaluator3.metricName:\u0027accuracy\u0027}))\nprint(evaluator3.evaluate(test_pred,{evaluator3.metricName:\u0027accuracy\u0027}))\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nimport pyspark.ml.clustering as clus\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfeaturesCreator\n"
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nkmeans\u003dclus.KMeans(k\u003d5,featuresCol\u003d\u0027features\u0027)\nclus_pip\u003dPipeline(stages\u003d[featuresCreator,kmeans])\nclus_model\u003dclus_pip.fit(births_train)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ntrain_pred\u003dclus_model.transform(births_train)\ntrain_pred.groupBy(\u0027prediction\u0027).count()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ntrain_cluster\u003dtrain_pred.groupBy(\u0027prediction\u0027).count()\ntrain_total\u003dtrain_pred.count()\ntrain_cluster.show()\ntrain_cluster\u003dtrain_cluster.withColumn(\u0027Perc\u0027,round(train_cluster[\u0027count\u0027]/train_total,3))\ntrain_cluster.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\npred\u003dclus_model.transform(births_test)\npred.select([\u0027prediction\u0027]).show(10)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\ntest_cluster\u003dpred.groupBy(\u0027prediction\u0027).count()\ntest_total\u003dpred.count()\ntest_cluster.show()\ntest_cluster\u003dtest_cluster.withColumn(\u0027Perc\u0027,round(test_cluster[\u0027count\u0027]/test_total,3))\ntest_cluster.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf \u003d spark.createDataFrame([(0, 33.3, -17.5),\n                              (1, 40.4, -20.5),\n                              (2, 28., -23.9),\n                              (3, 29.5, -19.0),\n                              (4, 32.8, -18.84)\n                             ],\n                              [\"other\",\"lat\", \"long\"])\n\ndf.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nprint(\u0027hello\u0027)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf_vec\u003dft.VectorAssembler(inputCols\u003d[\u0027lat\u0027,\u0027long\u0027],outputCol\u003d\u0027features\u0027)\ndf_kmean\u003dclus.KMeans(k\u003d2,featuresCol\u003d\u0027features\u0027)\ndf_pip\u003dPipeline(stages\u003d[df_vec,df_kmean])\ndf_kmodel\u003ddf_pip.fit(df)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nccFraud.agg(*[skewness(col).alias(col+\u0027_skew\u0027) for col in numerical]).show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nn_num\u003dlen(numerical)\ncorr\u003d[]\nfor i in range(n_num):\n    cor\u003d[ccFraud.corr(numerical[i],numerical[j]) for j in range(n_num) ]\n    corr.append(cor)\ncorr"
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nimport gc\ndel ccFraud\ngc.collect()"
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nccFraud.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf\u003dspark.sql(\"SELECT * FROM core_clickstream.daily LIMIT 10\")\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf.printSchema()\ndf.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf1\u003ddf.select(\u0027geo_city\u0027,\u0027geo_region\u0027,\u0027geo_zip\u0027,\u0027year\u0027)\ndf1.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nx.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf1 \u003d spark.createDataFrame([\n(1, 144.5, 5.9, 33, \u0027M\u0027),\n(2, 167.2, 5.4, 45, \u0027M\u0027),\n(3, 124.1, 5.2, 23, \u0027F\u0027),\n(4, 144.5, 5.9, 33, \u0027M\u0027),\n(5, 133.2, 5.7, 54, \u0027F\u0027),\n(3, 124.1, 5.2, 23, \u0027F\u0027),\n(5, 129.2, 5.3, 42, \u0027M\u0027),\n], [\u0027id\u0027, \u0027weight\u0027, \u0027height\u0027, \u0027age\u0027, \u0027gender\u0027])\n\n\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nprint(df1.count())\nprint(df1.distinct().count())\n"
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf\u003ddf.dropDuplicates()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf\u003ddf.dropDuplicates(subset\u003d[c for c in df.columns if c!\u003d\u0027id\u0027])\n"
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nimport pyspark.sql.functions as fn \n"
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf.agg(fn.count(\u0027id\u0027).alias(\u0027count\u0027),fn.countDistinct(\u0027id\u0027).alias(\u0027distinct\u0027)).show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf_miss \u003d spark.createDataFrame([\n(1, 143.5, 5.6, 28, \u0027M\u0027, 100000),\n(2, 167.2, 5.4, 45, \u0027M\u0027, None),\n(3, None , 5.2, None, None, None),\n(4, 144.5, 5.9, 33, \u0027M\u0027, None),\n(5, 133.2, 5.7, 54, \u0027F\u0027, None),\n(6, 124.1, 5.2, None, \u0027F\u0027, None),\n(7, 129.2, 5.3, 42, \u0027M\u0027, 76000),\n], [\u0027id\u0027, \u0027weight\u0027, \u0027height\u0027, \u0027age\u0027, \u0027gender\u0027, \u0027income\u0027])\ndf_miss.show()\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf_miss.printSchema()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf_miss[df_miss.age.isNull()].show()"
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf_miss.rdd.map(lambda row: (row[\u0027id\u0027],sum([c\u003d\u003dNone for c in row]))).collect()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf_miss.where(\u0027id\u003d\u003d3\u0027).show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf_miss.drop(\u0027income\u0027).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql.functions import *"
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf_miss.columns[1:4]"
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf_miss.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nnp.array(df_miss.columns)[df_miss.agg(*[(sum(col(c).isNull().cast(\u0027int\u0027))\u003e0).alias(c) for c in df_miss.columns]).toPandas().values.reshape(1,6)]\n"
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nnp.array(df_miss.columns).reshape(1,6)[df_miss.agg(*[(sum(col(c).isNull().cast(\u0027int\u0027))\u003e0).alias(c) for c in df_miss.columns]).toPandas().values.reshape(1,6)]"
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nimport numpy as np\nimport pandas as pd"
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nnp.array(df_miss.columns)[np.array([True,False,True,False,True,False])]"
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf_miss_noincome\u003ddf_miss.select([c for c in df_miss.columns if c!\u003d\u0027income\u0027])\n"
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf_miss_noincome.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nmedian\u003d{}\nfor var in df_miss_noincome.columns[1:4]:\n    med\u003ddf_miss_noincome.approxQuantile(var,[0.5],0.01)\n    median[var]\u003dmed[0]\nmedian"
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf_miss_noincome.fillna(median).show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nmeans\u003ddf_miss_noincome.agg(*[mean(c).alias(c) for c in df_miss_noincome.columns if c!\u003d\u0027gender\u0027]).toPandas().to_dict(\u0027record\u0027)[0]\nmeans\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf_miss_noincome.agg(*[col(c).isNull() for c in df_miss_noincome.columns if c!\u003d\u0027gender\u0027]).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf_gen_miss\u003ddf_miss_noincome.groupby(\u0027gender\u0027).count()"
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf_gen_miss.select(\u0027gender\u0027).filter(df_gen_miss[\u0027count\u0027]\u003d\u003dmax_count).collect()[0][\u0027gender\u0027]"
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nmax_count\u003ddf_gen_miss.agg(max(\u0027count\u0027).alias(\u0027max_count\u0027)).collect()[0][\u0027max_count\u0027]"
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf_miss_noincome.fillna(means).show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf_miss_noincome.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\ndf_outliers \u003d spark.createDataFrame([\n(1, 143.5, 5.3, 28),\n(2, 154.2, 5.5, 45),\n(3, 342.3, 5.1, 99),\n(4, 144.5, 5.5, 33),\n(5, 133.2, 5.4, 54),\n(6, 124.1, 5.1, 21),\n(7, 129.2, 5.3, 42),\n], [\u0027id\u0027, \u0027weight\u0027, \u0027height\u0027, \u0027age\u0027])\ndf_outliers.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nmedian\u003d{}\nfor var in df_outliers.columns[1:]:\n    q1\u003ddf_outliers.approxQuantile(var,[0.5],0.01)\n    median[var]\u003dq1\nmedian\n"
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ncols\u003ddf_outliers.columns[1:]\nbounds\u003d{}\nfor col in cols:\n    quantiles\u003ddf_outliers.approxQuantile(col,[0.25,0.75],0.05)\n    IQR\u003dquantiles[1]-quantiles[0]\n    bounds[col]\u003d[quantiles[0]-1.5*IQR,quantiles[1]+1.5*IQR]\nquantiles   \n"
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\noutliers\u003ddf_outliers.select(*[\u0027id\u0027]+[((df_outliers[c]\u003cbounds[c][0]) | (df_outliers[c]\u003ebounds[c][1])).alias(c+\u0027_o\u0027) for c in cols])\noutliers.show()\n\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf_outliers\u003ddf_outliers.join(outliers,on\u003d\u0027id\u0027)\ndf_outliers.filter(\u0027weight_o\u0027).select(\u0027id\u0027,\u0027weight\u0027).show()\ndf_outliers.filter(\u0027age_o\u0027).select(\u0027id\u0027,\u0027age\u0027).show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nwget http://tomdrabas.com/data/LearningPySpark/ccFraud.csv.gz\n"
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nimport pyspark.sql.types as typ\n"
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nlabels \u003d [\n    (\u0027INFANT_ALIVE_AT_REPORT\u0027, typ.StringType()),\n    (\u0027BIRTH_YEAR\u0027, typ.IntegerType()),\n    (\u0027BIRTH_MONTH\u0027, typ.IntegerType()),\n    (\u0027BIRTH_PLACE\u0027, typ.StringType()),\n    (\u0027MOTHER_AGE_YEARS\u0027, typ.IntegerType()),\n    (\u0027MOTHER_RACE_6CODE\u0027, typ.StringType()),\n    (\u0027MOTHER_EDUCATION\u0027, typ.StringType()),\n    (\u0027FATHER_COMBINED_AGE\u0027, typ.IntegerType()),\n    (\u0027FATHER_EDUCATION\u0027, typ.StringType()),\n    (\u0027MONTH_PRECARE_RECODE\u0027, typ.StringType()),\n    (\u0027CIG_BEFORE\u0027, typ.IntegerType()),\n    (\u0027CIG_1_TRI\u0027, typ.IntegerType()),\n    (\u0027CIG_2_TRI\u0027, typ.IntegerType()),\n    (\u0027CIG_3_TRI\u0027, typ.IntegerType()),\n    (\u0027MOTHER_HEIGHT_IN\u0027, typ.IntegerType()),\n    (\u0027MOTHER_BMI_RECODE\u0027, typ.IntegerType()),\n    (\u0027MOTHER_PRE_WEIGHT\u0027, typ.IntegerType()),\n    (\u0027MOTHER_DELIVERY_WEIGHT\u0027, typ.IntegerType()),\n    (\u0027MOTHER_WEIGHT_GAIN\u0027, typ.IntegerType()),\n    (\u0027DIABETES_PRE\u0027, typ.StringType()),\n    (\u0027DIABETES_GEST\u0027, typ.StringType()),\n    (\u0027HYP_TENS_PRE\u0027, typ.StringType()),\n    (\u0027HYP_TENS_GEST\u0027, typ.StringType()),\n    (\u0027PREV_BIRTH_PRETERM\u0027, typ.StringType()),\n    (\u0027NO_RISK\u0027, typ.StringType()),\n    (\u0027NO_INFECTIONS_REPORTED\u0027, typ.StringType()),\n    (\u0027LABOR_IND\u0027, typ.StringType()),\n    (\u0027LABOR_AUGM\u0027, typ.StringType()),\n    (\u0027STEROIDS\u0027, typ.StringType()),\n    (\u0027ANTIBIOTICS\u0027, typ.StringType()),\n    (\u0027ANESTHESIA\u0027, typ.StringType()),\n    (\u0027DELIV_METHOD_RECODE_COMB\u0027, typ.StringType()),\n    (\u0027ATTENDANT_BIRTH\u0027, typ.StringType()),\n    (\u0027APGAR_5\u0027, typ.IntegerType()),\n    (\u0027APGAR_5_RECODE\u0027, typ.StringType()),\n    (\u0027APGAR_10\u0027, typ.IntegerType()),\n    (\u0027APGAR_10_RECODE\u0027, typ.StringType()),\n    (\u0027INFANT_SEX\u0027, typ.StringType()),\n    (\u0027OBSTETRIC_GESTATION_WEEKS\u0027, typ.IntegerType()),\n    (\u0027INFANT_WEIGHT_GRAMS\u0027, typ.IntegerType()),\n    (\u0027INFANT_ASSIST_VENTI\u0027, typ.StringType()),\n    (\u0027INFANT_ASSIST_VENTI_6HRS\u0027, typ.StringType()),\n    (\u0027INFANT_NICU_ADMISSION\u0027, typ.StringType()),\n    (\u0027INFANT_SURFACANT\u0027, typ.StringType()),\n    (\u0027INFANT_ANTIBIOTICS\u0027, typ.StringType()),\n    (\u0027INFANT_SEIZURES\u0027, typ.StringType()),\n    (\u0027INFANT_NO_ABNORMALITIES\u0027, typ.StringType()),\n    (\u0027INFANT_ANCEPHALY\u0027, typ.StringType()),\n    (\u0027INFANT_MENINGOMYELOCELE\u0027, typ.StringType()),\n    (\u0027INFANT_LIMB_REDUCTION\u0027, typ.StringType()),\n    (\u0027INFANT_DOWN_SYNDROME\u0027, typ.StringType()),\n    (\u0027INFANT_SUSPECTED_CHROMOSOMAL_DISORDER\u0027, typ.StringType()),\n    (\u0027INFANT_NO_CONGENITAL_ANOMALIES_CHECKED\u0027, typ.StringType()),\n    (\u0027INFANT_BREASTFED\u0027, typ.StringType())\n]\nschema\u003dStructType([StructField(i[0],i[1],False) for i in labels])\n"
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nbirths2\u003dspark.read.csv(\u0027s3://lerawzone/users/gyang/births_train.csv.gz\u0027,header\u003dTrue,schema\u003dschema)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfeatures\u003d[i for i in births2.columns ]\nfeatures\n"
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nbirths2[features[20:]].dtypes\n"
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nselected_features \u003d [\u0027INFANT_ALIVE_AT_REPORT\u0027,\n\u0027BIRTH_PLACE\u0027,\n\u0027MOTHER_AGE_YEARS\u0027,\n\u0027FATHER_COMBINED_AGE\u0027,\n\u0027CIG_BEFORE\u0027,\n\u0027CIG_1_TRI\u0027,\n\u0027CIG_2_TRI\u0027,\n\u0027CIG_3_TRI\u0027,\n\u0027MOTHER_HEIGHT_IN\u0027,\n\u0027MOTHER_PRE_WEIGHT\u0027,\n\u0027MOTHER_DELIVERY_WEIGHT\u0027,\n\u0027MOTHER_WEIGHT_GAIN\u0027,\n\u0027DIABETES_PRE\u0027,\n\u0027DIABETES_GEST\u0027,\n\u0027HYP_TENS_PRE\u0027,\n\u0027HYP_TENS_GEST\u0027,\n\u0027PREV_BIRTH_PRETERM\u0027\n]\n"
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nbirths_trimmed\u003dbirths2[selected_features]\n"
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nbirths_trimmed.show()\nbirths_trimmed.dtypes\n"
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nrecode_dictionry\u003d{\u0027YNU\u0027:{\u0027Y\u0027:1,\u0027N\u0027:0,\"U\":0}}\ndef recode(col,key):\n    return recode_dictionary[key][col]\ndef correct_cig(feat):\n    return when(col(feat)!\u003d99,col(feat)).otherwise(0)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nrec_integer\u003dudf(recode,IntegerType())"
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nselected_features[4:8]\n"
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nbirths_trans\u003dbirths_trimmed.withColumn(\u0027CIG_BEFORE\u0027,when(births_trimmed.CIG_BEFORE!\u003d99,births_trimmed.CIG_BEFORE).otherwise(0))\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfor feat in selected_features[5:8]:\n    births_trans\u003dbirths_trans.withColumn(feat,correct_cig(feat))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nbirths_trans.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nbirths2.groupby(features[0]).agg(*[sum(i).alias(i+\u0027_sum\u0027) for i in births2[features[11:19]].columns]).show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nccFraud\u003dspark.read.csv(\u0027http://packages.revolutionanalytics.com/datasets/ccFraud.csv\u0027)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nonetime_buyers\u003dspark.read.csv(\u0027s3://lerawzone/users/gyang/onetime_clean3.csv\u0027,header\u003dTrue)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nonetime_buyers.count()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nonetime_buyers\u003donetime_buyers.drop(\u0027_c0\u0027)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nimport pyspark.ml.feature as ft\nfrom pyspark.sql.functions import *\nimport pyspark.ml.classification as cl\nimport pyspark.ml.evaluation as ev \nimport pandas as pd\nimport numpy as np \n"
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nonetime_buyers1\u003donetime_buyers.drop(\u0027TOTAL_C4DMD\u0027,\u0027MEN_C4DMD\u0027, \u0027MEN_C4ORDER\u0027, \u0027WM_C4DMD\u0027, \u0027WM_C4ORDER\u0027, \u0027KIDS_C4DMD\u0027,\n       \u0027KIDS_C4ORDER\u0027, \u0027HOME_C4DMD\u0027, \u0027HOME_C4ORDER\u0027, \u0027UNIFORM_C4DMD012\u0027,\u0027UNIFORM_C4ORDER012\u0027, \u0027OTHER_C4DMD\u0027, \u0027OTHER_C4ORDER\u0027,\n        \u0027HOME_ACC_UNITS\u0027,\u0027BED_UNITS\u0027, \u0027BATH_UNITS\u0027, \u0027WM_OUTERWEAR_UNITS\u0027,\n       \u0027WM_ACTIVEWEAR_UNITS\u0027, \u0027WM_SWEATERS_UNITS\u0027, \u0027WM_WOVEN_TOPS_UNITS\u0027,\n       \u0027WM_BOTTOMS_UNITS\u0027, \u0027WM_JACKETS_UNITS\u0027, \u0027WM_SLEEPWEAR_UNITS\u0027,\n       \u0027WM_KNIT_TOPS_UNITS\u0027, \u0027WM_DRESSES_UNITS\u0027, \u0027WM_SWIMWEAR_UNITS\u0027,\n       \u0027WXR_OUTERWEAR_UNITS\u0027, \u0027WXR_ACTIVEWEAR_UNITS\u0027, \u0027WXR_SWEATERS_UNITS\u0027,\n       \u0027WXR_WOVEN_TOPS_UNITS\u0027, \u0027WXR_BOTTOMS_UNITS\u0027, \u0027WXR_JACKETS_UNITS\u0027,\n       \u0027WXR_SLEEPWEAR_UNITS\u0027, \u0027WXR_KNIT_TOPS_UNITS\u0027, \u0027WXR_DRESSES_UNITS\u0027,\n       \u0027WXR_SWIMWEAR_UNITS\u0027, \u0027MN_SOFT_ACCESSORIES_UNITS\u0027,\n       \u0027MN_OUTERWEAR_UNITS\u0027, \u0027MN_SWIMWEAR_UNITS\u0027, \u0027MN_BOTTOMS_UNITS\u0027,\n       \u0027MN_SLEEPWEAR_UNITS\u0027, \u0027MN_WOVEN_TOPS_UNITS\u0027,\n       \u0027MN_DRESS_SHIRTS_UNITS\u0027, \u0027MN_CWA_UNITS\u0027, \u0027MN_SWEATERS_UNITS\u0027,\n       \u0027MN_ACTIVE_WEAR_UNITS\u0027, \u0027MN_KNIT_TOPS_UNITS\u0027,\n       \u0027MN_DRESS_SEPARATES_UNITS\u0027, \u0027WM_SOFT_ACC_UNITS\u0027, \u0027WM_BAGS_UNITS\u0027,\n       \u0027KIDS_FTWR_UNITS\u0027, \u0027WM_FTWR_UNITS\u0027, \u0027MN_FTWR_UNITS\u0027, \u0027WM_CWA_UNITS\u0027,\n       \u0027UNF_GIRLS_UNITS\u0027, \u0027UNF_BOYS_UNITS\u0027, \u0027UNF_COED_UNITS\u0027,\n       \u0027UNF_OUTERWEAR_UNITS\u0027, \u0027KIDS_BABY_UNITS\u0027, \u0027KIDS_SOFT_ACC_UNITS\u0027,\n       \u0027BOYS_BOTTOMS_UNITS\u0027, \u0027GIRLS_DRESSES_UNITS\u0027, \u0027KIDS_SLEEPWEAR_UNITS\u0027,\n       \u0027GIRLS_OUTERWEAR_UNITS\u0027, \u0027GIRLS_BOTTOMS_UNITS\u0027,\n       \u0027BOYS_OUTERWEAR_UNITS\u0027, \u0027BOYS_SWIMWEAR_UNITS\u0027, \u0027GIRLS_TOPS_UNITS\u0027,\n       \u0027BOYS_TOPS_UNITS\u0027, \u0027KIDS_BAGS_UNITS\u0027, \u0027GIRLS_SWIMEAR_UNITS\u0027,\n       \u0027KIDS_CWA_UNITS\u0027, \u0027LEBO_UNITS\u0027,\u0027KIDS_HOME_DEMAND\u0027,\n       \u0027HOME_ACC_DEMAND\u0027, \u0027BED_DEMAND\u0027, \u0027BATH_DEMAND\u0027,\n       \u0027WM_OUTERWEAR_DEMAND\u0027, \u0027WM_ACTIVEWEAR_DEMAND\u0027, \u0027WM_SWEATERS_DEMAND\u0027,\n       \u0027WM_WOVEN_TOPS_DEMAND\u0027, \u0027WM_BOTTOMS_DEMAND\u0027, \u0027WM_JACKETS_DEMAND\u0027,\n       \u0027WM_SLEEPWEAR_DEMAND\u0027, \u0027WM_KNIT_TOPS_DEMAND\u0027, \u0027WM_DRESSES_DEMAND\u0027,\n       \u0027WM_SWIMWEAR_DEMAND\u0027, \u0027WXR_OUTERWEAR_DEMAND\u0027,\n       \u0027WXR_ACTIVEWEAR_DEMAND\u0027, \u0027WXR_SWEATERS_DEMAND\u0027,\n       \u0027WXR_WOVEN_TOPS_DEMAND\u0027, \u0027WXR_BOTTOMS_DEMAND\u0027, \u0027WXR_JACKETS_DEMAND\u0027,\n       \u0027WXR_SLEEPWEAR_DEMAND\u0027, \u0027WXR_KNIT_TOPS_DEMAND\u0027,\n       \u0027WXR_DRESSES_DEMAND\u0027, \u0027WXR_SWIMWEAR_DEMAND\u0027,\n       \u0027MN_SOFT_ACCESSORIES_DEMAND\u0027, \u0027MN_OUTERWEAR_DEMAND\u0027,\n       \u0027MN_SWIMWEAR_DEMAND\u0027, \u0027MN_BOTTOMS_DEMAND\u0027, \u0027MN_SLEEPWEAR_DEMAND\u0027,\n       \u0027MN_WOVEN_TOPS_DEMAND\u0027, \u0027MN_DRESS_SHIRTS_DEMAND\u0027, \u0027MN_CWA_DEMAND\u0027,\n       \u0027MN_SWEATERS_DEMAND\u0027, \u0027MN_ACTIVE_WEAR_DEMAND\u0027,\n       \u0027MN_KNIT_TOPS_DEMAND\u0027, \u0027MN_DRESS_SEPARATES_DEMAND\u0027,\n       \u0027WM_SOFT_ACC_DEMAND\u0027, \u0027WM_BAGS_DEMAND\u0027, \u0027KIDS_FTWR_DEMAND\u0027,\n       \u0027WM_FTWR_DEMAND\u0027, \u0027MN_FTWR_DEMAND\u0027, \u0027WM_CWA_DEMAND\u0027,\n       \u0027UNF_GIRLS_DEMAND\u0027, \u0027UNF_BOYS_DEMAND\u0027, \u0027UNF_COED_DEMAND\u0027,\n       \u0027UNF_OUTERWEAR_DEMAND\u0027, \u0027KIDS_BABY_DEMAND\u0027, \u0027KIDS_SOFT_ACC_DEMAND\u0027,\n       \u0027BOYS_BOTTOMS_DEMAND\u0027, \u0027GIRLS_DRESSES_DEMAND\u0027,\n       \u0027KIDS_SLEEPWEAR_DEMAND\u0027, \u0027GIRLS_OUTERWEAR_DEMAND\u0027,\n       \u0027GIRLS_BOTTOMS_DEMAND\u0027, \u0027BOYS_OUTERWEAR_DEMAND\u0027,\n       \u0027BOYS_SWIMWEAR_DEMAND\u0027, \u0027GIRLS_TOPS_DEMAND\u0027, \u0027BOYS_TOPS_DEMAND\u0027,\n       \u0027KIDS_BAGS_DEMAND\u0027, \u0027GIRLS_SWIMEAR_DEMAND\u0027, \u0027KIDS_CWA_DEMAND\u0027,\n       \u0027LEBO_DEMAND\u0027)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfor column in onetime_buyers1.columns:\n    onetime_buyers1\u003donetime_buyers1.withColumn(column,col(column).cast(\u0027float\u0027))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nonetime_buyers1.columns\n"
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfeatures\u003donetime_buyers1.columns[1:-1]\nfeatureCreator\u003dft.VectorAssembler(inputCols\u003dfeatures,outputCol\u003d\u0027features\u0027)\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ntrain_ot1,test_ot1\u003donetime_buyers1.randomSplit([0.7,0.3],seed\u003d123)\n\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ntrain_ot1\u003dfeatureCreator.transform(train_ot1)\ntest_ot1\u003dfeatureCreator.transform(test_ot1)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nresult\u003dnp.zeros((6,4))\nm\u003d0\nfor i in range(3,6):\n    for j in range(50,70,10):\n        GB\u003dcl.GBTClassifier(maxDepth\u003di,labelCol\u003d\u0027resp\u0027,maxIter\u003dj)\n        onetimeModel\u003dGB.fit(train_ot)\n        train_pred\u003donetimeModel.transform(train_ot)\n        test_pred\u003donetimeModel.transform(test_ot)\n        GB_eval\u003dev.BinaryClassificationEvaluator(rawPredictionCol\u003d\u0027probability\u0027,labelCol\u003d\u0027resp\u0027)\n        result[m,0]\u003di\n        result[m,1]\u003dj\n        result[m,2]\u003dGB_eval.evaluate(train_pred,{GB_eval.metricName:\u0027areaUnderROC\u0027})\n        result[m,3]\u003dGB_eval.evaluate(test_pred,{GB_eval.metricName:\u0027areaUnderROC\u0027})\n        m+\u003d1\n\n    \n"
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nresult\n"
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nGB_best\u003dcl.GBTClassifier(maxDepth\u003d3,maxIter\u003d150,labelCol\u003d\u0027resp\u0027)\nonetime_best\u003dGB_best.fit(train_ot1)\ntrain_best\u003donetime_best.transform(train_ot1)\ntest_best\u003donetime_best.transform(test_ot1)\nGB_eval\u003dev.BinaryClassificationEvaluator(rawPredictionCol\u003d\u0027probability\u0027,labelCol\u003d\u0027resp\u0027)\nprint(GB_eval.evaluate(train_best,{GB_eval.metricName:\u0027areaUnderROC\u0027}))\nprint(GB_eval.evaluate(test_best,{GB_eval.metricName:\u0027areaUnderROC\u0027}))"
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nonetime_best.featureImportances\n"
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\npd.DataFrame({\u0027Features\u0027:features,\u0027Importance\u0027:onetime_best.featureImportances.values})"
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nimport tensorflow as tf\n"
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    }
  ]
}